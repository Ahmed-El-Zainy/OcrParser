# -*- coding: utf-8 -*-
"""embedding_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1euDHRgIf6vb1hWS9Rn2-il2DixSlY3Ne
"""



from google.colab import drive
drive.mount('/content/drive')

"""## OCR Models

#### Qari Model
"""

!pip install transformers qwen_vl_utils accelerate>=0.26.0 PEFT -U
!pip install -U bitsandbytes



from PIL import Image
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
import torch
import os
import glob
from qwen_vl_utils import process_vision_info

# Model setup
model_name = "NAMAA-Space/Qari-OCR-0.1-VL-2B-Instruct"
model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_name)
max_tokens = 2000

# Folder path
input_folder = "/content/drive/MyDrive/test_data"
output_folder = input_folder  # Save to the same folder

# OCR prompt
prompt = "Below is the image of one page of a document, as well as some raw textual content that was previously extracted for it. Just return the plain text representation of this document as if you were reading it naturally. Do not hallucinate."

# Get model name for output files (clean version)
clean_model_name = model_name.replace("/", "_").replace("-", "_")

# Supported image extensions
image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif', '*.webp']

# Get all image files in the folder
image_files = []
for extension in image_extensions:
    image_files.extend(glob.glob(os.path.join(input_folder, extension)))
    image_files.extend(glob.glob(os.path.join(input_folder, extension.upper())))

print(f"Found {len(image_files)} image files to process")

# Process each image
for i, image_path in enumerate(image_files):
    try:
        print(f"Processing image {i+1}/{len(image_files)}: {os.path.basename(image_path)}")

        # Prepare messages for the model
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},
                    {"type": "text", "text": prompt},
                ],
            }
        ]

        # Process the input
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to("cuda")

        # Generate OCR output
        generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        # Create output filename
        base_filename = os.path.splitext(os.path.basename(image_path))[0]
        output_filename = f"{base_filename}_{clean_model_name}_ocr.txt"
        output_path = os.path.join(output_folder, output_filename)

        # Save the output to text file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(output_text)

        print(f"‚úì Saved OCR result to: {output_filename}")
        print(f"Preview: {output_text[:100]}...")
        print("-" * 50)

    except Exception as e:
        print(f"‚úó Error processing {os.path.basename(image_path)}: {str(e)}")
        continue

print(f"\nProcessing complete! Processed {len(image_files)} images.")
print(f"Output files saved in: {output_folder}")

torch.cuda.empty_cache()

"""#### Nanonet's OCR"""

!pip install -U flash-attn --no-build-isolation

"""##### Using Doctest"""

# install the docext package
## UV tries to access these files
!mkdir /backend-container
!mkdir /backend-container/containers
!touch /backend-container/containers/build.constraints
!touch /backend-container/containers/requirements.constraints
!uv pip install docext -q
# run the app
!python -m docext.app.app --max_model_len 15000 --gpu_memory_utilization 0.9 --max_num_imgs 1 --max_img_size 1024 --concurrency_limit 1

# create a virtual environment
## install uv if not installed
!curl -LsSf https://astral.sh/uv/install.sh | sh
## create a virtual environment with python 3.11
!uv venv --python=3.11
!source .venv/bin/activate

# Install from PyPI
!uv pip install docext

# Or install from source
# !git clone https://github.com/nanonets/docext.git
# %cd docext
# !uv pip install -e .

"""- Start the API Server"""

# !python -m docext.app.app --concurrency_limit 10
!python -m docext.app.app --model_name hosted_vllm/nanonets/Nanonets-OCR-s

import time
from gradio_client import Client, handle_file

def convert_pdf_to_markdown(
    client_url: str,
    username: str,
    password: str,
    file_paths: list[str],
    model_name: str = "hosted_vllm/nanonets/Nanonets-OCR-s"
):
    """
    Convert PDF/images to markdown using the API

    Args:
        client_url: URL of the docext server
        username: Authentication username
        password: Authentication password
        file_paths: List of file paths to convert
        model_name: Model to use for conversion

    Returns:
        str: Converted markdown content
    """
    client = Client(client_url, auth=(username, password))

    # Prepare file inputs
    file_inputs = [{"image": handle_file(file_path)} for file_path in file_paths]

    # Convert to markdown (non-streaming)
    result = client.predict(
        images=file_inputs,
        api_name="/process_markdown_streaming"
    )

    return result

# Example usage
# client url can be the local host or the public url like `https://6986bdd23daef6f7eb.gradio.live/`
CLIENT_URL = "http://localhost:7860"

# Single image conversion
markdown_content = convert_pdf_to_markdown(
    CLIENT_URL,
    "admin",
    "admin",
    ["assets/invoice_test.pdf"]
)
print(markdown_content)

# Multiple files conversion
markdown_content = convert_pdf_to_markdown(
    CLIENT_URL,
    "admin",
    "admin",
    ["assets/invoice_test.jpeg", "assets/invoice_test.pdf"]
)
print(markdown_content)

# Start the web interface with default configs
# !python -m docext.app.app --model_name hosted_vllm/nanonets/Nanonets-OCR-s

# Start the web interface with custom configs
# !python -m docext.app.app --model_name hosted_vllm/nanonets/Nanonets-OCR-s --max_img_size 1024 --concurrency_limit 16 # `--help` for more options

"""##### Using HuggingFace Instllations"""

!gdown -qqq 18jwp7X3wFoVTq75Y4ARjXbgFEVPUwOHm
!unzip -qq ocr-documents.zip

!pip install -Uqqq pip --progress-bar off
!pip install -qqq transformers==4.52.4 --progress-bar off
!pip install -qqq flash-attn==2.7.4.post1 --progress-bar off
!pip install -qqq rich==14.0.0 --progress-bar off

from PIL import Image
from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText
from rich.console import Console
from rich.markdown import Markdown
import base64
from markdown import markdown
from IPython.display import display, HTML

model_path = "nanonets/Nanonets-OCR-s"

model = AutoModelForImageTextToText.from_pretrained(
    model_path,
    torch_dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.eval()

tokenizer = AutoTokenizer.from_pretrained(model_path)
processor = AutoProcessor.from_pretrained(model_path)

def ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=4096):
    prompt = """Extract the text from the above document as if you were reading it naturally. Return the tables in markdown format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the  tag; otherwise, add the image caption inside . Watermarks should be wrapped in brackets. Ex: OFFICIAL COPY. Page numbers should be wrapped in brackets. Ex: 14 or 9/22. Prefer using ‚òê and ‚òë for check boxes."""
    image = Image.open(image_path)
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": [
                {"type": "image", "image": f"file://{image_path}"},
                {"type": "text", "text": prompt},
            ],
        },
    ]
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = processor(text=[text], images=[image], padding=True, return_tensors="pt")
    inputs = inputs.to(model.device)

    output_ids = model.generate(
        **inputs, max_new_tokens=max_new_tokens, do_sample=False
    )
    generated_ids = [
        output_ids[len(input_ids) :]
        for input_ids, output_ids in zip(inputs.input_ids, output_ids)
    ]

    output_text = processor.batch_decode(
        generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
    )
    return output_text[0]

# Commented out IPython magic to ensure Python compatibility.
# 
# %%time
# image_path = "/content/drive/MyDrive/test_data/page_001_img_063.png"
# result = ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=15000)
# 
# console = Console()
# console.print(Markdown(result))

from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
import os
import glob

# Model setup
model_path = "nanonets/Nanonets-OCR-s"


# processor = AutoProcessor.from_pretrained("nanonets/Nanonets-OCR-s")
# model = AutoModelForImageTextToText.from_pretrained("nanonets/Nanonets-OCR-s")

# Folder path
input_folder = "/content/drive/MyDrive/test_data"
output_folder = input_folder  # Save to the same folder

# Get model name for output files (clean version)
clean_model_name = model_path.replace("/", "_").replace("-", "_")

# OCR function
def ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=4096):
    prompt = """‚ÄúPlease extract all Arabic text from this digital page,
    preserving the correct reading order and punctuation. Also,
    if there are any labels, annotations, or legends within charts or graphs,
     extract and group them separately. Do not attempt to describe the visuals‚Äîonly return the actual text as seen.
     The output should be clean, readable Arabic.‚Äù"""

    image = Image.open(image_path)
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": [
            {"type": "image", "image": f"file://{image_path}"},
            {"type": "text", "text": prompt},
        ]},
    ]

    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[image], padding=True, return_tensors="pt")
    inputs = inputs.to(model.device)

    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]

    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return output_text[0]

# Supported image extensions
image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif', '*.webp']

# Get all image files in the folder
image_files = []
for extension in image_extensions:
    image_files.extend(glob.glob(os.path.join(input_folder, extension)))
    image_files.extend(glob.glob(os.path.join(input_folder, extension.upper())))

print(f"Found {len(image_files)} image files to process")

# Process each image
for i, image_path in enumerate(image_files):
    try:
        print(f"Processing image {i+1}/{len(image_files)}: {os.path.basename(image_path)}")

        # Run OCR on the image
        result = ocr_page_with_nanonets_s(image_path, model, processor, max_new_tokens=15000)

        # Create output filename
        base_filename = os.path.splitext(os.path.basename(image_path))[0]
        output_filename = f"{base_filename}_{clean_model_name}_ocr.txt"
        output_path = os.path.join(output_folder, output_filename)

        # Save the output to text file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(result)

        print(f"‚úì Saved OCR result to: {output_filename}")
        print(f"Preview: {result[:100]}...")
        print("-" * 50)

    except Exception as e:
        print(f"‚úó Error processing {os.path.basename(image_path)}: {str(e)}")
        continue

print(f"\nProcessing complete! Processed {len(image_files)} images.")
print(f"Output files saved in: {output_folder}")

"""#### Dots OCR"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/rednote-hilab/dots.ocr.git
# %cd dots.ocr
!pip install -q -U torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
!pip install -q -U vllm
!pip install -e .
!python tools/download_model.py

import os
from pathlib import Path

# Set up model path (using a directory in Colab's temporary storage)
hf_model_path = "./weights/DotsOCR"
os.environ["hf_model_path"] = hf_model_path

# Create directory if it doesn't exist
Path(hf_model_path).mkdir(parents=True, exist_ok=True)

# Add to PYTHONPATH
os.environ["PYTHONPATH"] = f"{os.path.dirname(hf_model_path)}:{os.environ.get('PYTHONPATH', '')}"

# Modify vllm import (this is a workaround - may need adjustment based on vllm version)
try:
    vllm_path = !which vllm
    if vllm_path:
        vllm_path = vllm_path[0]
        !sed -i '/^from vllm\.entrypoints\.cli\.main import main$/a from DotsOCR import modeling_dots_ocr_vllm' {vllm_path}
except:
    print("Could not automatically modify vllm imports. You may need to do this manually.")

# Parse all layout info, both detection and recognition
# Parse a single image
# python3 dots_ocr/parser.py demo/demo_image1.jpg
# Parse a single PDF
# !python3 dots_ocr/parser.py demo/demo_pdf1.pdf  --num_thread 64  # try bigger num_threads for pdf with a large number of pages

# # Layout detection only
# python3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_layout_only_en

# # Parse text only, except Page-header and Page-footer
# python3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_ocr

# # Parse layout info by bbox
# python3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_grounding_ocr --bbox 163 241 1536 705

"""*doc_utils*

* gradio demo
"""

!python tools/download_model.py

# Commented out IPython magic to ensure Python compatibility.
# %ls

# %cd /content/dots.ocr
!python dots_ocr/parser.py demo/demo_image1.jpg

!python demo/demo_gradio.py --share

import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")

import transformers
import flash_attn
print(f"Transformers version: {transformers.__version__}")
print(f"Flash Attn version with python 3.11 , cuda 12.4 and torch 2.6: {flash_attn.__version__}")
!python --version

# Commented out IPython magic to ensure Python compatibility.
# %ls

!pip freeze > updates_req.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd dots.ocr/
# %ls

!pip install gdown



# !python3 demo/demo_hf.py

import torch
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer
from qwen_vl_utils import process_vision_info
from dots_ocr.utils import dict_promptmode_to_prompt

model_path = "./weights/DotsOCR"
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    # attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)
processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)

image_path = "demo/demo_image1.jpg"
prompt = """Please output the layout information from the PDF image, including each layout element's bbox, its category, and the corresponding text content within the bbox.

1. Bbox format: [x1, y1, x2, y2]

2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].

3. Text Extraction & Formatting Rules:
    - Picture: For the 'Picture' category, the text field should be omitted.
    - Formula: Format its text as LaTeX.
    - Table: Format its text as HTML.
    - All Others (Text, Title, etc.): Format their text as Markdown.

4. Constraints:
    - The output text must be the original text from the image, with no translation.
    - All layout elements must be sorted according to human reading order.

5. Final Output: The entire output must be a single JSON object.
"""

messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image_path
                },
                {"type": "text", "text": prompt}
            ]
        }
    ]

# Preparation for inference
text = processor.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to("cuda")
# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=24000)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)
print(output_text)

import torch
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer
from qwen_vl_utils import process_vision_info
from dots_ocr.utils import dict_promptmode_to_prompt
import warnings
import os
import glob
import json

warnings.filterwarnings("ignore")

# Model setup
model_path = "./weights/DotsOCR"
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    # attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
    # local_files_only=True
)
processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)

# Folder path
input_folder = "/content/dots.ocr/ÿßŸÑÿßŸÖÿ™ÿ≠ÿßŸÜ ŸÅŸäÿ≤Ÿäÿßÿ° Ÿ£ÿ´ ÿßÿ≥ÿ¶ŸÑÿ© Ÿ¢Ÿ†Ÿ¢Ÿ¶.pdf"
output_folder = input_folder  # Save to the same folder


import fitz
import numpy as np
import enum
from pydantic import BaseModel, Field
from PIL import Image


class SupportedPdfParseMethod(enum.Enum):
    OCR = 'ocr'
    TXT = 'txt'

class PageInfo(BaseModel):
    w: float = Field(description='the width of page')
    h: float = Field(description='the height of page')


def fitz_doc_to_image(doc, target_dpi=200, origin_dpi=None) -> dict:
    from PIL import Image
    mat = fitz.Matrix(target_dpi / 72, target_dpi / 72)
    pm = doc.get_pixmap(matrix=mat, alpha=False)
    if pm.width > 4500 or pm.height > 4500:
        mat = fitz.Matrix(72 / 72, 72 / 72)  # use fitz default dpi
        pm = doc.get_pixmap(matrix=mat, alpha=False)
    image = Image.frombytes('RGB', (pm.width, pm.height), pm.samples)
    return image


def load_images_from_pdf(pdf_file, dpi=200, start_page_id=0, end_page_id=None) -> list:
    images = []
    with fitz.open(pdf_file) as doc:
        pdf_page_num = doc.page_count
        end_page_id = (
            end_page_id
            if end_page_id is not None and end_page_id >= 0
            else pdf_page_num - 1)
        if end_page_id > pdf_page_num - 1:
            print('end_page_id is out of range, use images length')
            end_page_id = pdf_page_num - 1
        for index in range(0, doc.page_count):
            if start_page_id <= index <= end_page_id:
                page = doc[index]
                img = fitz_doc_to_image(page, target_dpi=dpi)
                images.append(img)
    return images


clean_model_name = "DotsOCR"
prompts = {

    "txt": """Please extract all text content from the PDF image and return it as plain text. Follow these rules:
    - Extract text in reading order
    - Use simple line breaks for paragraphs
    - Convert tables to simple text format
    - Convert formulas to plain text representation
    - Include all textual content without formatting markup
    - Do not include layout or positioning information""",

    "md": """Please extract the content from the PDF image and format it as clean Markdown. Follow these rules:
    - Use proper Markdown syntax for headers (# ## ###)
    - Format tables using Markdown table syntax
    - Use **bold** and *italic* for emphasis where appropriate
    - Convert formulas to LaTeX format wrapped in $ or $$
    - Use proper list formatting (- or 1.)
    - Preserve document hierarchy and structure
    - Use code blocks for any code content""",

    "json": """Please output the layout information from the PDF image as a structured JSON object, including each layout element's bbox, its category, and the corresponding text content within the bbox.
1. Bbox format: [x1, y1, x2, y2]
2. Layout Categories: The possible categories are ['Caption', 'Footnote', 'Formula', 'List-item', 'Page-footer', 'Page-header', 'Picture', 'Section-header', 'Table', 'Text', 'Title'].
3. Text Extraction & Formatting Rules:
    - Picture: For the 'Picture' category, the text field should be omitted.
    - Formula: Format its text as LaTeX.
    - Table: Format its text as HTML.
    - All Others (Text, Title, etc.): Format their text as Markdown.

4. Constraints:
    - The output text must be the original text from the image, with no translation.
    - All layout elements must be sorted according to human reading order.

5. Final Output: The entire output must be a single JSON object."""
}


# prompts = {
#     "txt": """Please extract all text content from the PDF image and return it as plain text. Follow these rules:

#     - Extract text in human reading order.
#     - Use simple line breaks for paragraphs.
#     - Convert tables to simple text rows.
#     - Convert formulas to plain text representation.
#     - Include all textual content without layout markup.
#     - Do not include layout/position styling other than explicit bbox information when available.

#     ADDITIONAL RULES FOR QUESTIONS & ANSWERS MAPPING:
#     1. Detect question blocks that start with a bracketed number pattern like "[1]" or "(1)". For each detected question:
#       - Create a question tag: Q<n> where <n> is the bracketed number (e.g., Q1 for [1]).
#       - Create a unique question id (UID): Q<n>_<shortuid> (shortuid = 6-8 alphanumeric characters). If the same <n> repeats, append _1, _2, etc.
#       - Output a question header line immediately before the question text in this exact format:
#         >>>QUESTION id:<UID> tag:Q<n> number:[<n>] page:<pageno> bbox:[x1,y1,x2,y2]
#       - Then output the question text and any choices on following lines.

#     2. Detect the answers section (commonly at the end of the PDF). For each answer that starts with the same bracketed style "[1]":
#       - Create an answer tag: A<n>, and a unique answer id UID: A<n>_<shortuid>.
#       - Output an answer header line in this exact format:
#         >>>ANSWER id:<UID> tag:A<n> number:[<n>] page:<pageno> bbox:[x1,y1,x2,y2]
#       - Then output the answer text on the following line(s).

#     3. After full extraction, append a machine-friendly mapping block (exact text between triple dashes) that lists every question UID ‚Üí its corresponding answer UID and the raw texts, like:
#     ---
#     MAPPING:
#     QUID: Q1_ABC123 -> AUID: A1_DEF456  | question_number: [1] | question_page: 2 | answer_page: 10
#     QUID: Q2_XYZ789 -> AUID: A2_GHI012  | question_number: [2] | question_page: 3 | answer_page: 10
#     ---
#     If an answer for a question is not found, still list the QUID with AUID: null.

#     4. If a question is present but not numbered, generate a tag QGEN<0001> and a UID QGEN0001_<shortuid>.

#     5. Preserve original characters (no translation) and keep the output as plain text only.
#     """,

#     "md": """
#     Please extract the content from the PDF image and format it as clean Markdown. Follow these rules:

# - Use proper Markdown headers (#, ##, ###) for document structure.
# - Format tables using Markdown table syntax.
# - Use **bold** and *italic* where emphasis is present.
# - Convert formulas to LaTeX format wrapped in `$` or `$$`.
# - Use proper list formatting (`-` or `1.`).
# - Preserve document hierarchy and structure.
# - Use fenced code blocks for any code content.
# - Do not translate or alter original text.

# ADDITIONAL RULES FOR QUESTIONS & ANSWERS MAPPING:
# 1. For each detected question that begins with a numbered marker like `[1]`:
#    - Create a header for that question in this format:
#      ## Question [1] ‚Äî tag: **Q1** ‚Äî id: `Q1_<shortuid>`  (page: <pageno>, bbox: [x1,y1,x2,y2])
#    - Immediately below the header include the question text and choices (as bullet list or numbered list).

# 2. Produce an **Answers** section near the end of the Markdown. For each detected answer starting with `[1]`, use this format:
#    ## Answer [1] ‚Äî tag: **A1** ‚Äî id: `A1_<shortuid>` (page: <pageno>, bbox: [x1,y1,x2,y2])
#    - Then include the answer text.

# 3. Add a machine-friendly mapping code block at the very end:
# \```json
# {
#   "mapping": {
#     "Q1_<shortuid>": "A1_<shortuid>",
#     "Q2_<shortuid>": null
#   },
#   "question_index": [
#     {"qid":"Q1_<shortuid>", "number":1, "page":2, "bbox":[x1,y1,x2,y2]},
#     {"qid":"Q2_<shortuid>", "number":2, "page":3, "bbox":[x1,y1,x2,y2]}
#   ],
#   "answer_index": [
#     {"aid":"A1_<shortuid>", "number":1, "page":10, "bbox":[x1,y1,x2,y2]}
#   ]
# }
# \```

# 4. If questions are unnumbered, generate tags like `QGEN0001_<shortuid>` and include them in the mapping.

# 5. Keep the Markdown human-readable while including the exact tags/IDs needed for programmatic parsing.

#     """,

#     "json": """
#     Please output the layout information from the PDF image as a single structured JSON object. Include each layout element's bbox, its category, and the corresponding text content within the bbox.

# 1. Bbox format: [x1, y1, x2, y2] (numbers in PDF coordinate units).
# 2. Layout Categories allowed: ['Caption','Footnote','Formula','List-item','Page-footer','Page-header','Picture','Section-header','Table','Text','Title','Question','Answer'].
#    - For 'Picture' omit the text field.
#    - For 'Formula' format text as LaTeX.
#    - For 'Table' format text as HTML.
#    - For 'Question' and 'Answer' include structured fields described below.
# 3. Text extraction & formatting:
#    - Preserve original characters (no translation).
#    - All textual fields should be raw extracted text (not HTML-encoded), except tables (HTML) and formulas (LaTeX).

# QUESTION & ANSWER JSON SCHEMA (required):
# - Output a single JSON object with these top-level keys:
#   {
#     "document": {"pages": <int>, "source": "<filename-or-null>"},
#     "elements": [ ... ],
#     "qa_mapping": { "<question_uid>": "<answer_uid or null>", ... }
#   }

# - Each element in "elements" must be an object with:
#   {
#     "type": "Question" | "Answer" | other layout categories,
#     "text": "<raw text or formatted representation>",
#     "page": <page_number>,
#     "bbox": [x1,y1,x2,y2],
#     // For Questions:
#     "qnum": <int or null>,            // the bracketed number if present (e.g. 1 for [1])
#     "tag": "Q<n>" or "QGEN0001",
#     "uid": "Q<n>_<shortuid>",
#     "choices": ["choice A", "choice B", ...] // empty if none
#     // For Answers:
#     "anum": <int or null>,
#     "tag": "A<n>",
#     "uid": "A<n>_<shortuid>",
#     "qid": "<the question uid this answer maps to (if determinable)>"
#   }

# - Rules for uid/tag generation:
#   * If question starts with [n], set qnum=n and tag "Qn" and uid "Qn_<shortuid>".
#   * If question is unnumbered, set tag "QGEN<seq>" and uid "QGEN<seq>_<shortuid>".
#   * shortuid: 6-8 alphanumeric deterministic token (if possible) or random.

# - Sorting & reading order:
#   * The "elements" array must be sorted in human reading order.
#   * If an Answer appears in the document body, include it in elements too; but ensure the final answers block (commonly at the end) is correctly identified and included.

# - Final qa_mapping:
#   * An explicit object mapping question UID -> answer UID (or null) must appear at top-level "qa_mapping".
#   Example:
#   "qa_mapping": {
#     "Q1_ABC123": "A1_DEF456",
#     "Q2_XYZ789": null
#   }

# 4. Final Output Constraint:
# - The output must be a single JSON object only (no extra text outside the JSON).
# - All bounding boxes and page numbers should be included where available.

#     """


# }



# Supported image extensions
image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.tif', '*.webp']

# Get all image files in the folder
image_files = []
for extension in image_extensions:
    image_files.extend(glob.glob(os.path.join(input_folder, extension)))
    image_files.extend(glob.glob(os.path.join(input_folder, extension.upper())))

print(f"Found {len(image_files)} image files to process")

# Process each image with all three formats
for i, image_path in enumerate(image_files):
    try:
        print(f"Processing image {i+1}/{len(image_files)}: {os.path.basename(image_path)}")
        base_filename = os.path.splitext(os.path.basename(image_path))[0]

        # Process for each output format
        for format_type, prompt in prompts.items():
            try:
                print(f"  Generating {format_type.upper()} format...")
                # Prepare messages for the model
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "image": image_path
                            },
                            {"type": "text", "text": prompt}
                        ]
                    }
                ]

                # Preparation for inference
                text = processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                image_inputs, video_inputs = process_vision_info(messages)
                inputs = processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt",
                )

                inputs = inputs.to("cuda")

                # Inference: Generation of the output
                generated_ids = model.generate(**inputs, max_new_tokens=24000)
                generated_ids_trimmed = [
                    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                output_text = processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )

                # Extract the output text (it's a list, get first element)
                result = output_text[0] if output_text else ""

                # Create output filename
                output_filename = f"{base_filename}_{clean_model_name}.{format_type}"
                output_path = os.path.join(output_folder, output_filename)

                # Save based on format type
                if format_type == "json":
                    try:
                        # Try to parse as JSON and save with proper formatting
                        parsed_json = json.loads(result)
                        with open(output_path, 'w', encoding='utf-8') as f:
                            json.dump(parsed_json, f, indent=2, ensure_ascii=False)
                        print(f"    ‚úì Saved as formatted JSON: {output_filename}")
                    except json.JSONDecodeError:
                        # If not valid JSON, save as text file with .json extension
                        with open(output_path, 'w', encoding='utf-8') as f:
                            f.write(result)
                        print(f"    ‚úì Saved as text (invalid JSON): {output_filename}")
                else:
                    # Save as plain text or markdown
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(result)
                    print(f"    ‚úì Saved as {format_type.upper()}: {output_filename}")

            except Exception as format_error:
                print(f"    ‚úó Error generating {format_type.upper()} format: {str(format_error)}")
                continue

        # Show preview of one format (txt)
        if 'result' in locals():
            preview = result[:150] if len(result) > 150 else result
            print(f"Preview: {preview}...")

        print("-" * 60)

    except Exception as e:
        print(f"‚úó Error processing {os.path.basename(image_path)}: {str(e)}")
        continue

print(f"\nProcessing complete! Processed {len(image_files)} images.")
print(f"Output files saved in: {output_folder}")
print("\nFile naming convention:")
print(f"- Plain text: {{filename}}_{clean_model_name}.txt")
print(f"- Markdown: {{filename}}_{clean_model_name}.md")
print(f"- JSON: {{filename}}_{clean_model_name}.json")

!pip install transformers==4.51.3

# Commented out IPython magic to ensure Python compatibility.
# %cd dots.ocr/
# %ls











!gdown 1AQxkR-86lEw07lm7i8RD92RvOoaisLdw

import torch
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer
from qwen_vl_utils import process_vision_info
import warnings
import os
import glob
import json
import fitz  # PyMuPDF
from PIL import Image
import time
from tqdm import tqdm
import logging

# Suppress all warnings and logs
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("torch").setLevel(logging.ERROR)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# =============================================================================
# CONFIGURATION SECTION - UPDATE THESE SETTINGS
# =============================================================================

# Model setup
model_path = "./weights/DotsOCR"

# File paths
input_path = "/content/dots.ocr/ÿßŸÑÿßŸÖÿ™ÿ≠ÿßŸÜ ŸÅŸäÿ≤Ÿäÿßÿ° Ÿ£ÿ´ ÿßÿ≥ÿ¶ŸÑÿ© Ÿ¢Ÿ†Ÿ¢Ÿ¶.pdf"  # Single file or folder
output_folder = "/content/output/"  # Where to save outputs

# PDF processing settings
target_dpi = 200  # DPI for PDF to image conversion
max_dimension = 4500  # Maximum image dimension

# Output settings
SHOW_PROCESSING_DETAILS = True  # Show detailed processing info for each image
SHOW_TEXT_PREVIEW = True  # Show preview of extracted text
PREVIEW_LENGTH = 300  # Number of characters to show in preview
SHOW_IMAGE_INFO = True  # Show image dimensions and file info

# Progress tracking
class ProgressTracker:
    def __init__(self):
        self.current_step = 0
        self.total_steps = 0
        self.current_task = ""
        self.progress_bar = None

    def start_process(self, total_steps, description="Processing"):
        self.total_steps = total_steps
        self.current_step = 0
        self.progress_bar = tqdm(total=total_steps, desc=description,
                                unit="step", ncols=100,
                                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')

    def update(self, step_name="", increment=1):
        if self.progress_bar:
            self.current_step += increment
            self.progress_bar.set_postfix_str(step_name[:40])
            self.progress_bar.update(increment)

    def complete(self):
        if self.progress_bar:
            self.progress_bar.close()

# Global progress tracker
progress = ProgressTracker()

# =============================================================================

# DotsOCR Model setup
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,)
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    print("‚úÖ DotsOCR model loaded successfully")
except Exception as e:
    print(f"‚ùå Error loading DotsOCR model: {e}")
    exit(1)

def fitz_doc_to_image(page, target_dpi=200):
    try:
        mat = fitz.Matrix(target_dpi / 72, target_dpi / 72)
        pm = page.get_pixmap(matrix=mat, alpha=False)
        if pm.width > max_dimension or pm.height > max_dimension:
            mat = fitz.Matrix(72 / 72, 72 / 72)
            pm = page.get_pixmap(matrix=mat, alpha=False)
        image = Image.frombytes('RGB', (pm.width, pm.height), pm.samples)
        return image, pm.width, pm.height
    except:
        return None, 0, 0

def load_images_from_pdf(pdf_file, dpi=200, start_page_id=0, end_page_id=None):
    """Load images from PDF file"""
    images = []
    print(f"\nüìñ Loading PDF: {os.path.basename(pdf_file)}")

    try:
        with fitz.open(pdf_file) as doc:
            pdf_page_num = doc.page_count
            end_page_id = end_page_id if end_page_id is not None else pdf_page_num - 1
            end_page_id = min(end_page_id, pdf_page_num - 1)

            print(f"üìÑ Total pages in PDF: {pdf_page_num}")
            print(f"üìÑ Processing pages {start_page_id + 1} to {end_page_id + 1}")

            for index in range(start_page_id, end_page_id + 1):
                page = doc[index]
                img, width, height = fitz_doc_to_image(page, target_dpi=dpi)
                if img:
                    images.append({
                        'image': img,
                        'page_num': index + 1,
                        'width': width,
                        'height': height
                    })
                    if SHOW_IMAGE_INFO:
                        print(f"  üñºÔ∏è  Page {index + 1}: {width}x{height} pixels")

    except Exception as e:
        print(f"‚ùå Error loading PDF: {e}")

    print(f"‚úÖ Successfully loaded {len(images)} pages")
    return images

def process_single_image_with_model(image, prompt, page_num=1, width=0, height=0):
    """Process a single image with the DotsOCR model"""
    processing_start_time = time.time()

    if SHOW_PROCESSING_DETAILS:
        print(f"\nüîç Processing Page {page_num}")
        print(f"   üìè Image size: {width}x{height} pixels")
        print(f"   üß† Using DotsOCR model...")

    try:
        temp_path = f"/tmp/temp_page_{page_num}.png"
        image.save(temp_path, optimize=True, quality=85)

        if SHOW_PROCESSING_DETAILS:
            file_size = os.path.getsize(temp_path) / 1024  # KB
            print(f"   üíæ Temp image saved: {file_size:.1f} KB")

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": temp_path},
                    {"type": "text", "text": prompt}
                ]
            }
        ]

        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )

        inputs = inputs.to("cuda")

        # Suppress tokenizer warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=6144,
                do_sample=True,
                temperature=0.1,
                top_p=0.9,
                pad_token_id=processor.tokenizer.eos_token_id
            )

        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )

        # Clean up
        if os.path.exists(temp_path):
            os.remove(temp_path)

        processing_time = time.time() - processing_start_time
        result_text = output_text[0] if output_text else ""

        if SHOW_PROCESSING_DETAILS:
            print(f"   ‚è±Ô∏è  Processing time: {processing_time:.2f} seconds")
            print(f"   üìù Extracted {len(result_text)} characters")

        if SHOW_TEXT_PREVIEW and result_text.strip():
            preview_text = result_text.strip()
            if len(preview_text) > PREVIEW_LENGTH:
                preview_text = preview_text[:PREVIEW_LENGTH] + "..."
            print(f"   üìñ Text preview:")
            print(f"   ‚îî‚îÄ {preview_text.replace(chr(10), ' ').replace(chr(13), ' ')}")
        elif SHOW_TEXT_PREVIEW:
            print(f"   ‚ö†Ô∏è  No text extracted from this page")

        return result_text

    except Exception as e:
        if SHOW_PROCESSING_DETAILS:
            print(f"   ‚ùå Error processing page {page_num}: {e}")
        return ""

def clean_extracted_text(text):
    """Clean and normalize extracted text"""
    if not text:
        return ""

    # Basic text cleaning
    text = text.strip()

    # Remove excessive whitespace while preserving Arabic text structure
    import re
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)  # Reduce multiple newlines
    text = re.sub(r'[ \t]+', ' ', text)  # Normalize spaces and tabs

    return text

def save_as_txt(content, filename, output_folder):
    """Save content as TXT file"""
    txt_file = os.path.join(output_folder, f"{filename}.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"üíæ Saved TXT file: {os.path.basename(txt_file)} ({len(content)} characters)")
    return txt_file

def save_as_json(pages_data, filename, output_folder):
    """Save content as JSON file with page structure"""
    json_data = {
        "document_info": {
            "filename": filename,
            "extraction_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "extraction_method": "DotsOCR",
            "total_pages": len(pages_data),
            "total_characters": sum(len(page["text"]) for page in pages_data)
        },
        "pages": pages_data
    }

    json_file = os.path.join(output_folder, f"{filename}.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)

    file_size = os.path.getsize(json_file) / 1024  # KB
    print(f"üíæ Saved JSON file: {os.path.basename(json_file)} ({file_size:.1f} KB)")
    return json_file

def save_as_markdown(pages_data, filename, output_folder):
    """Save content as Markdown file"""
    md_content = f"# {filename}\n\n"
    md_content += f"**Extraction Date:** {time.strftime('%Y-%m-%d %H:%M:%S')}  \n"
    md_content += f"**Total Pages:** {len(pages_data)}  \n"
    md_content += f"**Extraction Method:** DotsOCR  \n\n"
    md_content += "---\n\n"

    for page_data in pages_data:
        page_num = page_data["page_number"]
        text = page_data["text"]
        char_count = page_data["character_count"]

        md_content += f"## Page {page_num}\n\n"
        md_content += f"*Characters: {char_count}*\n\n"

        if text.strip():
            md_content += f"{text}\n\n"
        else:
            md_content += "*No text extracted from this page*\n\n"

        md_content += "---\n\n"

    md_file = os.path.join(output_folder, f"{filename}.md")
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)

    file_size = os.path.getsize(md_file) / 1024  # KB
    print(f"üíæ Saved Markdown file: {os.path.basename(md_file)} ({file_size:.1f} KB)")
    return md_file

def process_file_sync(file_path):
    """Process file with text extraction only"""
    start_time = time.time()
    base_filename = os.path.splitext(os.path.basename(file_path))[0]
    file_ext = os.path.splitext(file_path)[1].lower()

    print(f"\n" + "="*80)
    print(f"üöÄ Starting extraction for: {base_filename}")
    print(f"üìÅ File path: {file_path}")
    print(f"üîß File type: {file_ext.upper()}")
    print("="*80)

    if file_ext == '.pdf':
        # Load PDF pages
        page_images = load_images_from_pdf(file_path, dpi=target_dpi)

        if not page_images:
            print("‚ùå No pages could be extracted from PDF")
            return

        # For very large PDFs, inform user about processing time
        if len(page_images) > 50:
            print(f"‚ö†Ô∏è  Large PDF detected ({len(page_images)} pages). This may take a while...")
            print("‚è±Ô∏è  Processing approximately 3-5 seconds per page...")
            estimated_time = len(page_images) * 4 / 60
            print(f"üïí Estimated time: {estimated_time:.1f} minutes")

        # Start progress tracking
        total_steps = len(page_images) + 3  # pages + save operations
        progress.start_process(total_steps, f"Extracting text from {base_filename}")

        # Text extraction prompt
        extraction_prompt = """Extract ALL text from this image exactly as it appears.
        Preserve the original formatting, line breaks, and text structure.
        Pay attention to:
        1. Arabic text and maintain its proper direction
        2. Numbers and mathematical expressions
        3. Headers, titles, and section divisions
        4. Lists and bullet points
        5. Any English text mixed with Arabic

        Extract everything visible in the image as accurately as possible."""

        # Extract text from all pages
        pages_data = []
        full_text_content = ""

        for i, page_data in enumerate(page_images, 1):
            page_img = page_data['image']
            page_num = page_data['page_num']
            width = page_data['width']
            height = page_data['height']

            progress.update(f"Page {page_num}/{len(page_images)}")

            page_text = process_single_image_with_model(
                page_img, extraction_prompt, page_num, width, height
            )
            cleaned_text = clean_extracted_text(page_text)

            page_info = {
                "page_number": page_num,
                "text": cleaned_text,
                "character_count": len(cleaned_text),
                "has_content": bool(cleaned_text.strip()),
                "image_dimensions": f"{width}x{height}"
            }

            pages_data.append(page_info)
            full_text_content += f"--- Page {page_num} ---\n{cleaned_text}\n\n"

        print(f"\nüìä Processing completed! Summary:")

        # Save in multiple formats
        progress.update("Saving as TXT")
        txt_file = save_as_txt(full_text_content, base_filename, output_folder)

        progress.update("Saving as JSON")
        json_file = save_as_json(pages_data, base_filename, output_folder)

        progress.update("Saving as Markdown")
        md_file = save_as_markdown(pages_data, base_filename, output_folder)

        progress.complete()

        # Calculate statistics
        total_chars = sum(page["character_count"] for page in pages_data)
        pages_with_content = sum(1 for page in pages_data if page["has_content"])
        pages_empty = len(pages_data) - pages_with_content
        total_processing_time = time.time() - start_time

        # Display final summary
        print(f"\n" + "="*60)
        print(f"üìà EXTRACTION SUMMARY FOR {base_filename}")
        print(f"="*60)
        print(f"üìÑ Total pages processed: {len(page_images)}")
        print(f"‚úÖ Pages with content: {pages_with_content}")
        print(f"‚ùå Empty pages: {pages_empty}")
        print(f"üìù Total characters extracted: {total_chars:,}")
        print(f"üìä Average characters per page: {total_chars // len(pages_data):,}")
        print(f"‚è±Ô∏è  Total processing time: {total_processing_time:.2f} seconds")
        print(f"‚ö° Average time per page: {total_processing_time / len(page_images):.2f} seconds")

        print(f"\nüìÅ Output Files:")
        print(f"   üìÑ TXT: {os.path.basename(txt_file)}")
        print(f"   üìã JSON: {os.path.basename(json_file)}")
        print(f"   üìù MD: {os.path.basename(md_file)}")

        # Show sample of extracted text
        if full_text_content.strip():
            sample_text = full_text_content.strip()[:300]
            if len(full_text_content) > 300:
                sample_text += "..."
            print(f"\nüìñ Sample extracted text:")
            print(f"   {'-'*50}")
            print(f"   {sample_text}")
            print(f"   {'-'*50}")
        else:
            print(f"\n‚ö†Ô∏è  No text content was extracted from any pages")

        print(f"="*60)

def process_files_sync():
    """Main processing function"""
    print(f"üìÅ Creating output folder: {output_folder}")
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
        print("‚úÖ Output folder created")
    else:
        print("üìÅ Output folder already exists")

    if os.path.isfile(input_path):
        print(f"üéØ Processing single file: {os.path.basename(input_path)}")
        process_file_sync(input_path)
    elif os.path.isdir(input_path):
        pdf_files = glob.glob(os.path.join(input_path, "*.pdf"))
        print(f"üìÇ Found {len(pdf_files)} PDF files to process in directory")

        for i, pdf_file in enumerate(pdf_files, 1):
            print(f"\n{'='*20} FILE {i}/{len(pdf_files)} {'='*20}")
            process_file_sync(pdf_file)
    else:
        print(f"‚ùå Input path does not exist: {input_path}")
        return

    print(f"\nüéâ Text extraction complete!")
    print(f"üìÅ Output folder: {output_folder}")

def main():
    """Main execution function"""
    print("üî§ PDF Text Extractor with DotsOCR")
    print("=" * 50)
    print("‚ú® Features:")
    print("   ‚Ä¢ DotsOCR for accurate Arabic text extraction")
    print("   ‚Ä¢ Multiple output formats (TXT, JSON, MD)")
    print("   ‚Ä¢ Detailed progress tracking and verbose output")
    print("   ‚Ä¢ Preserves original text structure")
    print("   ‚Ä¢ Real-time processing feedback")
    print()

    print("üîß Configuration:")
    print(f"   ‚Ä¢ Target DPI: {target_dpi}")
    print(f"   ‚Ä¢ Max dimension: {max_dimension}")
    print(f"   ‚Ä¢ Show processing details: {SHOW_PROCESSING_DETAILS}")
    print(f"   ‚Ä¢ Show text preview: {SHOW_TEXT_PREVIEW}")
    print(f"   ‚Ä¢ Preview length: {PREVIEW_LENGTH} characters")
    print()

    # Check if models are loaded
    if 'model' not in globals():
        print("‚ùå Error: DotsOCR model not loaded")
        return

    print("ü§ñ DotsOCR model ready for processing")
    print()
    process_files_sync()

if __name__ == "__main__":
    main()













import torch
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer
from qwen_vl_utils import process_vision_info
import warnings
import os
import glob
import json
import fitz  # PyMuPDF
from PIL import Image
import time
from tqdm import tqdm
import logging

# Suppress all warnings and logs
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("torch").setLevel(logging.ERROR)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# =============================================================================
# CONFIGURATION SECTION - UPDATE THESE SETTINGS
# =============================================================================

# Model setup
model_path = "./weights/DotsOCR"

# File paths
input_path = "/content/dots.ocr/ÿßŸÑÿßŸÖÿ™ÿ≠ÿßŸÜ ŸÅŸäÿ≤Ÿäÿßÿ° Ÿ£ÿ´ ÿßÿ≥ÿ¶ŸÑÿ© Ÿ¢Ÿ†Ÿ¢Ÿ¶.pdf"  # Single file or folder
output_folder = "/content/output/"  # Where to save outputs

# PDF processing settings
target_dpi = 150  # Reduced DPI for faster processing (was 200)
max_dimension = 3500  # Reduced max dimension for faster processing (was 4500)
batch_size = 4  # Process multiple pages in batch for efficiency
fast_mode = True  # Enable optimizations for speed over maximum accuracy

# Progress tracking
class ProgressTracker:
    def __init__(self):
        self.current_step = 0
        self.total_steps = 0
        self.current_task = ""
        self.progress_bar = None

    def start_process(self, total_steps, description="Processing"):
        self.total_steps = total_steps
        self.current_step = 0
        self.progress_bar = tqdm(total=total_steps, desc=description,
                                unit="step", ncols=100,
                                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')

    def update(self, step_name="", increment=1):
        if self.progress_bar:
            self.current_step += increment
            self.progress_bar.set_postfix_str(step_name[:40])
            self.progress_bar.update(increment)

    def complete(self):
        if self.progress_bar:
            self.progress_bar.close()

# Global progress tracker
progress = ProgressTracker()

# =============================================================================

# DotsOCR Model setup
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,)
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
except Exception as e:
    print(f"Error loading DotsOCR model: {e}")
    exit(1)

def fitz_doc_to_image(page, target_dpi=200):
    try:
        mat = fitz.Matrix(target_dpi / 72, target_dpi / 72)
        pm = page.get_pixmap(matrix=mat, alpha=False)
        if pm.width > max_dimension or pm.height > max_dimension:
            mat = fitz.Matrix(72 / 72, 72 / 72)
            pm = page.get_pixmap(matrix=mat, alpha=False)
        image = Image.frombytes('RGB', (pm.width, pm.height), pm.samples)
        return image
    except:
        return None

def load_images_from_pdf(pdf_file, dpi=200, start_page_id=0, end_page_id=None):
    """Load images from PDF file"""
    images = []
    try:
        with fitz.open(pdf_file) as doc:
            pdf_page_num = doc.page_count
            end_page_id = end_page_id if end_page_id is not None else pdf_page_num - 1
            end_page_id = min(end_page_id, pdf_page_num - 1)

            for index in range(start_page_id, end_page_id + 1):
                page = doc[index]
                img = fitz_doc_to_image(page, target_dpi=dpi)
                if img:
                    images.append({'image': img, 'page_num': index + 1})
    except:
        pass
    return images

def process_batch_images_with_model(image_batch, prompt):
    """Process multiple images in a batch for better efficiency"""
    results = []

    try:
        # Prepare all images
        temp_paths = []
        messages_batch = []

        for i, (image, page_num) in enumerate(image_batch):
            temp_path = f"/tmp/temp_batch_{i}_page_{page_num}.png"
            temp_paths.append(temp_path)

            # Resize image if too large for faster processing
            if fast_mode:
                img_width, img_height = image.size
                if img_width > 2000 or img_height > 2000:
                    ratio = min(2000/img_width, 2000/img_height)
                    new_size = (int(img_width * ratio), int(img_height * ratio))
                    image = image.resize(new_size, Image.Resampling.LANCZOS)

            image.save(temp_path, optimize=True, quality=75 if fast_mode else 85)

            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": temp_path},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            messages_batch.append(messages)

        # Process batch
        for i, (messages, (image, page_num)) in enumerate(zip(messages_batch, image_batch)):
            try:
                text = processor.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
                image_inputs, video_inputs = process_vision_info(messages)
                inputs = processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt",
                )

                inputs = inputs.to("cuda")

                # Faster generation settings for batch processing
                max_tokens = 4096 if fast_mode else 6144
                temp = 0.05 if fast_mode else 0.1

                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    generated_ids = model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=temp,
                        top_p=0.9,
                        pad_token_id=processor.tokenizer.eos_token_id
                    )

                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                output_text = processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )

                results.append(output_text[0] if output_text else "")

            except Exception as e:
                results.append("")

        # Cleanup
        for temp_path in temp_paths:
            if os.path.exists(temp_path):
                os.remove(temp_path)

    except Exception as e:
        results = [""] * len(image_batch)

    return results

def clean_extracted_text(text):
    """Clean and normalize extracted text"""
    if not text:
        return ""

    # Basic text cleaning
    text = text.strip()

    # Remove excessive whitespace while preserving Arabic text structure
    import re
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)  # Reduce multiple newlines
    text = re.sub(r'[ \t]+', ' ', text)  # Normalize spaces and tabs

    return text

def save_as_txt(content, filename, output_folder):
    """Save content as TXT file"""
    txt_file = os.path.join(output_folder, f"{filename}.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write(content)
    return txt_file

def save_as_json(pages_data, filename, output_folder):
    """Save content as JSON file with page structure"""
    json_data = {
        "document_info": {
            "filename": filename,
            "extraction_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "extraction_method": "DotsOCR",
            "total_pages": len(pages_data),
            "total_characters": sum(len(page["text"]) for page in pages_data)
        },
        "pages": pages_data
    }

    json_file = os.path.join(output_folder, f"{filename}.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    return json_file

def save_as_markdown(pages_data, filename, output_folder):
    """Save content as Markdown file"""
    md_content = f"# {filename}\n\n"
    md_content += f"**Extraction Date:** {time.strftime('%Y-%m-%d %H:%M:%S')}  \n"
    md_content += f"**Total Pages:** {len(pages_data)}  \n"
    md_content += f"**Extraction Method:** DotsOCR  \n\n"
    md_content += "---\n\n"

    for page_data in pages_data:
        page_num = page_data["page_number"]
        text = page_data["text"]
        char_count = page_data["character_count"]

        md_content += f"## Page {page_num}\n\n"
        md_content += f"*Characters: {char_count}*\n\n"

        if text.strip():
            md_content += f"{text}\n\n"
        else:
            md_content += "*No text extracted from this page*\n\n"

        md_content += "---\n\n"

    md_file = os.path.join(output_folder, f"{filename}.md")
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    return md_file

def process_file_sync(file_path):
    """Process file with text extraction only"""
    base_filename = os.path.splitext(os.path.basename(file_path))[0]
    file_ext = os.path.splitext(file_path)[1].lower()

    if file_ext == '.pdf':
        # Load PDF pages
        page_images = load_images_from_pdf(file_path, dpi=target_dpi)

        if not page_images:
            print("No pages could be extracted from PDF")
            return

        # For very large PDFs, inform user about processing time
        if len(page_images) > 50:
            print(f"Large PDF detected ({len(page_images)} pages). This may take a while...")
            if fast_mode:
                estimated_time = len(page_images) * 2 / 60  # Faster processing
                print("Fast mode enabled - processing approximately 2 seconds per page...")
            else:
                estimated_time = len(page_images) * 4 / 60
                print("Processing approximately 3-5 seconds per page...")
            print(f"Estimated time: {estimated_time:.1f} minutes")
            print(f"Batch processing enabled ({batch_size} pages at a time)")

        # Start progress tracking
        total_batches = (len(page_images) + batch_size - 1) // batch_size
        progress.start_process(total_batches + 3, f"Extracting text from {base_filename}")

        # Text extraction prompt - simplified for speed in fast mode
        if fast_mode:
            extraction_prompt = """Extract all text from this image. Focus on:
            1. All Arabic and English text
            2. Numbers and important content
            3. Maintain basic structure

            Be fast and accurate."""
        else:
            extraction_prompt = """Extract ALL text from this image exactly as it appears.
            Preserve the original formatting, line breaks, and text structure.
            Pay attention to:
            1. Arabic text and maintain its proper direction
            2. Numbers and mathematical expressions
            3. Headers, titles, and section divisions
            4. Lists and bullet points
            5. Any English text mixed with Arabic

            Extract everything visible in the image as accurately as possible."""

        # Extract text from all pages using batch processing
        pages_data = []
        full_text_content = ""

        for batch_start in range(0, len(page_images), batch_size):
            batch_end = min(batch_start + batch_size, len(page_images))
            current_batch = page_images[batch_start:batch_end]

            batch_num = (batch_start // batch_size) + 1
            total_batches = (len(page_images) + batch_size - 1) // batch_size
            progress.update(f"Batch {batch_num}/{total_batches} (pages {batch_start+1}-{batch_end})")

            # Prepare batch data
            image_batch = [(page_data['image'], page_data['page_num']) for page_data in current_batch]

            # Process batch
            batch_texts = process_batch_images_with_model(image_batch, extraction_prompt)

            # Store results
            for i, (page_data, extracted_text) in enumerate(zip(current_batch, batch_texts)):
                page_num = page_data['page_num']
                cleaned_text = clean_extracted_text(extracted_text)

                page_info = {
                    "page_number": page_num,
                    "text": cleaned_text,
                    "character_count": len(cleaned_text),
                    "has_content": bool(cleaned_text.strip())
                }

                pages_data.append(page_info)
                full_text_content += f"--- Page {page_num} ---\n{cleaned_text}\n\n"

        # Save in multiple formats
        progress.update("Saving as TXT")
        txt_file = save_as_txt(full_text_content, base_filename, output_folder)

        progress.update("Saving as JSON")
        json_file = save_as_json(pages_data, base_filename, output_folder)

        progress.update("Saving as Markdown")
        md_file = save_as_markdown(pages_data, base_filename, output_folder)

        progress.complete()

        # Calculate statistics
        total_chars = sum(page["character_count"] for page in pages_data)
        pages_with_content = sum(1 for page in pages_data if page["has_content"])
        pages_empty = len(pages_data) - pages_with_content

        # Display final summary
        print(f"\nExtraction Summary for {base_filename}:")
        print(f"  Total pages processed: {len(page_images)}")
        print(f"  Pages with content: {pages_with_content}")
        print(f"  Empty pages: {pages_empty}")
        print(f"  Total characters extracted: {total_chars:,}")
        print(f"  Average characters per page: {total_chars // len(pages_data):,}")

        print(f"\nFiles saved:")
        print(f"  TXT: {os.path.basename(txt_file)}")
        print(f"  JSON: {os.path.basename(json_file)}")
        print(f"  MD: {os.path.basename(md_file)}")

        # Show sample of extracted text
        if full_text_content.strip():
            sample_text = full_text_content.strip()[:200] + "..." if len(full_text_content) > 200 else full_text_content.strip()
            print(f"\nSample extracted text:")
            print(f"  {sample_text}")

def process_files_sync():
    """Main processing function"""
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    if os.path.isfile(input_path):
        process_file_sync(input_path)
    elif os.path.isdir(input_path):
        pdf_files = glob.glob(os.path.join(input_path, "*.pdf"))
        print(f"Found {len(pdf_files)} PDF files to process")
        for pdf_file in pdf_files:
            process_file_sync(pdf_file)
    else:
        print(f"Input path does not exist: {input_path}")
        return

    print("\nText extraction complete!")
    print(f"Output folder: {output_folder}")

def main():
    """Main execution function"""
    print("PDF Text Extractor")
    print("=" * 50)
    print("Features:")
    print("  ‚Ä¢ DotsOCR for accurate Arabic text extraction")
    print("  ‚Ä¢ Multiple output formats (TXT, JSON, MD)")
    print("  ‚Ä¢ Batch processing for improved speed")
    print("  ‚Ä¢ Fast mode optimization available")
    print()

    # Configuration summary
    print("Current Settings:")
    print(f"  ‚Ä¢ DPI: {target_dpi}")
    print(f"  ‚Ä¢ Max dimension: {max_dimension}")
    print(f"  ‚Ä¢ Batch size: {batch_size}")
    print(f"  ‚Ä¢ Fast mode: {'Enabled' if fast_mode else 'Disabled'}")
    print()

    # Check if models are loaded
    if 'model' not in globals():
        print("Error: DotsOCR model not loaded")
        return

    print("DotsOCR model loaded successfully")
    print()

    # Speed tip
    if not fast_mode:
        print("üí° Tip: For faster processing, set fast_mode = True in the configuration")
        print("   This trades some accuracy for ~50% speed improvement")
        print()
    process_files_sync()

if __name__ == "__main__":
    main()







!pip install langextract

import langextract as lx
import textwrap
import os
from getpass import getpass
from IPython.display import display, HTML


# Set up your Gemini API key
# Get your key from: https://aistudio.google.com/app/apikey
if 'GEMINI_API_KEY' not in os.environ:
    os.environ['GEMINI_API_KEY'] = getpass('Enter your Gemini API key: ')

# Define the extraction task with an improved prompt for Q&A mapping
prompt = textwrap.dedent("""\
    Extract related sentecnce, questions, answer choices, and correct answers from Arabic educational text.
    Also identify and explicitly state relationships that connect questions to their choices and correct answers.

    Rules:
    1. Extract exact text for questions, choices, and answers. Do not paraphrase.
    2. Identify and extract the question number/marker (like [Ÿ°], Ÿ°], etc.).
    3. Extract all available multiple choice options (ÿ£-, ÿ®-, ÿ¨-, ÿØ-, etc.).
    4. Find and extract the correct answer, including its number/marker and the correct choice letter.
    5. Create clear relationships between each question, its choices, and its correct answer.

    Focus on preserving Arabic text formatting and numbering systems.

    Example:
    Input Text:
    --- Page 1 ---

    ## Ÿ°] ŸÜŸÅŸáŸÖ ŸÖŸÜ ÿßŸÑŸÅŸÇÿ±ÿ© ÿßŸÑÿ£ŸàŸÑŸâ ÿ£ŸÜ ÿßŸÑÿ≥ÿ®ÿ® ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ŸÑÿ≠ÿØŸàÿ´ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ŸáŸà:

    ÿ£- ÿßŸÑÿ™ÿ¥ŸÇŸÇÿßÿ™ ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ©.
    ÿ®- ÿßŸÑÿ™ŸÇŸÑÿµÿßÿ™ ŸàÿßŸÑÿ∂ÿ∫Ÿàÿ∑ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ŸÅŸä ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.
    ÿ¨- ÿßŸÑÿ•ÿ≤ÿßÿ≠ÿ© ÿßŸÑÿπŸÖŸàÿØŸäÿ© ÿ£Ÿà ÿßŸÑÿ£ŸÅŸÇŸäŸá ÿ®ŸäŸÜ ÿßŸÑÿµÿÆŸàÿ±.
    ÿØ- ÿßŸÑŸÉŸÖŸäÿßÿ™ ÿßŸÑŸáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ŸàŸÑÿØÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.

    --- Page 10 ---
    ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©
    [Ÿ°] ÿØ- ÿßŸÑŸÉŸÖŸäÿßÿ™ ÿßŸÑŸáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ŸàŸÑÿØÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.

    Expected Extractions:
    [
      {
        "extraction_class": "question",
        "extraction_text": "Ÿ°] ŸÜŸÅŸáŸÖ ŸÖŸÜ ÿßŸÑŸÅŸÇÿ±ÿ© ÿßŸÑÿ£ŸàŸÑŸâ ÿ£ŸÜ ÿßŸÑÿ≥ÿ®ÿ® ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ŸÑÿ≠ÿØŸàÿ´ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ŸáŸà:",
        "attributes": {
          "question_number": "Ÿ°"
        }
      },
      {
        "extraction_class": "choices",
        "extraction_text": "ÿ£- ÿßŸÑÿ™ÿ¥ŸÇŸÇÿßÿ™ ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ©.\nÿ®- ÿßŸÑÿ™ŸÇŸÑÿµÿßÿ™ ŸàÿßŸÑÿ∂ÿ∫Ÿàÿ∑ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ŸÅŸä ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.\nÿ¨- ÿßŸÑÿ•ÿ≤ÿßÿ≠ÿ© ÿßŸÑÿπŸÖŸàÿØŸäÿ© ÿ£Ÿà ÿßŸÑÿ£ŸÅŸÇŸäŸá ÿ®ŸäŸÜ ÿßŸÑÿµÿÆŸàÿ±.\nÿØ- ÿßŸÑŸÉŸÖŸäÿßÿ™ ÿßŸÑŸáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ŸàŸÑÿØÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.",
        "attributes": {
          "question_number": "Ÿ°",
          "choice_labels": ["ÿ£", "ÿ®", "ÿ¨", "ÿØ"]
        }
      },
      {
        "extraction_class": "answer",
        "extraction_text": "[Ÿ°] ÿØ- ÿßŸÑŸÉŸÖŸäÿßÿ™ ÿßŸÑŸáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ŸàŸÑÿØÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.",
        "attributes": {
          "question_number": "Ÿ°",
          "correct_choice": "ÿØ"
        }
      },
      {
        "extraction_class": "relationship",
        "extraction_text": "Question Ÿ° is mapped to choices ÿ£-ÿØ and answer [Ÿ°] ÿØ",
        "attributes": {
          "type": "question_answer_mapping",
          "question_number": "Ÿ°",
          "correct_answer_choice": "ÿØ"
        }
      }
    ]
    """)



# Provide comprehensive examples (using the improved structure)
examples = [
    lx.data.ExampleData(
        text="""

--- Page 1 ---
ÿßŸÇÿ±ÿ£ ÿ´ŸÖ ÿ£ÿ¨ÿ®:

Ÿ°. ÿ™ÿπÿ±ŸÅ ÿßŸÑŸáÿ≤ÿßÿ™ ÿπŸÑŸâ ÿ£ŸÜŸáÿß ÿ∏ÿßŸáÿ±ÿ© ŸÉŸàŸÜŸäÿ© ŸÅŸäÿ≤Ÿäÿßÿ¶Ÿäÿ© ÿ®ÿßŸÑÿ∫ÿ© ÿßŸÑÿ™ÿπŸÇŸäÿØÿå ÿ™ÿ∏Ÿáÿ± ŸÉÿ≠ÿ±ŸÉÿßÿ™ ÿπÿ¥Ÿàÿßÿ¶Ÿäÿ© ŸÑŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿπŸÑŸâ ÿ¥ŸÉŸÑ ÿßÿ±ÿ™ÿπÿßÿ¥ Ÿàÿ™ÿ≠ÿ±ŸÉ Ÿàÿ™ŸÖŸàÿ¨ ÿπŸÜŸäŸÅÿå Ÿàÿ∞ŸÑŸÉ ŸÜÿ™Ÿäÿ¨ÿ© ÿ•ÿ∑ŸÑÿßŸÇ ŸÉŸÖŸäÿßÿ™ Ÿáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂ÿå ŸàŸáÿ∞Ÿá ÿßŸÑÿ∑ÿßŸÇÿ© ÿ™ÿ™ŸàŸÑÿØ ŸÜÿ™Ÿäÿ¨ÿ© ŸÑÿ•ÿ≤ÿßÿ≠ÿ© ÿπŸÖŸàÿØŸäÿ© ÿ£Ÿà ÿ£ŸÅŸÇŸäÿ© ÿ®ŸäŸÜ ÿµÿÆŸàÿ± ÿßŸÑÿ£ÿ±ÿ∂ ÿπÿ®ÿ± ÿßŸÑÿµÿØŸàÿπ ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿØÿ´ ŸÑÿ™ÿπÿ±ÿ∂Ÿáÿß ÿßŸÑŸÖÿ≥ÿ™ŸÖÿ± ŸÑŸÑÿ™ŸÇŸÑÿµÿßÿ™ ŸàÿßŸÑÿ∂ÿ∫Ÿàÿ∑ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ©.

Ÿ¢. ŸàŸäÿπÿßÿ∏ŸÖ ÿ™ÿ£ÿ´Ÿäÿ± ÿßŸÑŸáÿ≤ÿßÿ™ ŸÅŸä ÿßŸÑÿ£ÿ±ÿßÿ∂Ÿä ÿßŸÑÿ∂ÿßÿ∫ŸäŸÅÿ© ÿÆÿµŸàÿµÿß ŸÅŸä ÿßŸÑÿ±Ÿàÿßÿ≥ÿ® ÿßŸÑÿ±ŸÖŸÑŸäÿ© ŸàÿßŸÑÿ∑ŸäŸÜŸäÿ© ÿ≠ÿØŸäÿ´ÿ© ÿßŸÑÿ™ŸÉŸàŸäŸÜÿå ŸàŸäÿπŸÑŸÑ ÿ∞ŸÑŸÉ ÿ®ÿ£ŸÜ Ÿáÿ∞Ÿá ÿßŸÑÿ±Ÿàÿßÿ≥ÿ® ÿ™Ÿáÿ™ÿ≤ ÿ®ÿπŸÜŸÅ ÿ®ÿ≥ÿ®ÿ® ÿßŸÜÿÆŸÅÿßÿ∂ ŸÖÿπÿßŸÖŸÑ ŸÖÿ±ŸàŸÜÿ™Ÿáÿß ŸàÿµŸÑÿßÿ®ÿ™Ÿáÿß ŸàÿπÿØŸÖ ŸÖŸÇÿØÿ±ÿ™Ÿáÿß ÿπŸÑŸâ ÿ™ÿÆŸÅŸäÿ∂ ÿßŸÑÿ™ÿ£ÿ´Ÿäÿ± ÿßŸÑÿ™ÿ≥ÿßÿ±ÿπŸä ÿßŸÑÿ∞Ÿä ÿ™ÿ™ÿπÿ±ÿ∂ ŸÑŸá ÿßŸÑÿ≠ÿ®Ÿäÿ®ÿßÿ™ ÿ®ŸÅÿπŸÑ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ.

Ÿ£. ŸàŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿßŸÑÿ™Ÿä ÿ™ŸÇÿπ ŸÖÿ±ÿßŸÉÿ≤Ÿáÿß ÿßŸÑÿ≥ÿ∑ÿ≠Ÿäÿ© ŸÅŸä ŸÇÿßÿπ ÿßŸÑÿ®ÿ≠ÿßÿ± ÿ£Ÿà ÿßŸÑŸÖÿ≠Ÿäÿ∑ÿßÿ™ ŸÅŸÇÿØ ÿ™ÿ§ÿØŸä ÿ•ŸÑŸâ ÿ≠ÿØŸàÿ´ ÿ£ŸÖŸàÿßÿ¨ ŸÖÿßÿ¶Ÿäÿ© ÿ∂ÿÆŸÖÿ© ÿ¨ÿØÿß ÿ™ÿ≥ŸÖŸâ ÿßŸÑÿ™ÿ≥ŸàŸÜÿßŸÖŸä ŸàŸáŸä ŸÉŸÑŸÖÿ© Ÿäÿßÿ®ÿßŸÜŸäÿ© ŸÖÿπŸÜÿßŸáÿß ÿ£ŸÖŸàÿßÿ¨ ÿßŸÑŸÖŸàÿßŸÜÿ¶ ÿ£Ÿà ÿßŸÑÿÆŸÑÿ¨ÿßŸÜÿå ÿ•ÿ∞ ÿ™ÿ§ÿØŸä ÿßŸÑÿßŸáÿ™ÿ≤ÿßÿ≤ÿßÿ™ ÿßŸÑŸÖÿµÿßÿ≠ÿ®ÿ© ŸÑÿ≠ÿØŸàÿ´ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿ•ŸÑŸâ ÿ™ŸÉŸàŸÜ Ÿáÿ∞Ÿá ÿßŸÑÿ£ŸÖŸàÿßÿ¨ÿå ŸàŸÇÿØ ÿ™ÿµŸÑ ÿ≥ÿ±ÿπÿ™Ÿáÿß ÿ•ŸÑŸâ Ÿ®Ÿ†Ÿ† ŸÉŸÖ/ÿ≥ÿßÿπÿ©ÿå Ÿàÿ∞ŸÑŸÉ ŸÜÿ™Ÿäÿ¨ÿ© ŸÑÿßŸÜÿ≤ŸÑÿßŸÇ ÿµŸÅÿßÿ¶ÿ≠ ÿßŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿπŸÖŸàÿØŸäÿß ÿ®ÿπÿ∂Ÿáÿß ÿπŸÑŸâ ÿ®ÿπÿ∂ÿå ŸàŸÖÿß Ÿäÿ¨ÿØÿ± ÿ∞ŸÉÿ±Ÿá ŸáŸÜÿß ÿ£ŸÜ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿßŸÑÿ™Ÿä ÿ™ŸÜÿ¥ÿ£ ÿπŸÜ ÿßŸÜÿ≤ŸÑÿßŸÇÿß ÿ£ŸÅŸÇŸäÿ© ŸÅŸä ÿßŸÑÿµŸÅÿßÿ¶ÿ≠ ŸÑÿß ÿ™ÿ§ÿØŸä ÿ•ŸÑŸâ ÿ™ŸÉŸàŸÜ ÿ£ŸÖŸàÿßÿ¨ ÿßŸÑÿ™ÿ≥ŸàŸÜÿßŸÖŸä.

ÿ£. ŸàÿπŸÖŸàŸÖÿß ÿ™ŸÜÿ¥ÿ£ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿßŸÑÿ™ŸÉÿ™ŸàŸÜŸäÿ© ŸÜÿ™Ÿäÿ¨ÿ© ŸÑŸÑÿ≠ÿ±ŸÉÿ© ÿßŸÑŸÜÿ≥ÿ®Ÿäÿ© ŸÑŸÑÿµŸÅÿßÿ¶ÿ≠ - ÿßŸÑŸÇÿ∑ÿπ ÿßŸÑÿ™Ÿä ÿ™ÿ™ÿ¥ŸÉŸÑ ŸÖŸÜŸáÿß ÿßŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ©. ÿ•ÿ∞ ÿ™ÿ™ÿ≠ÿ±ŸÉ ÿßŸÑŸÇÿßÿ±ÿßÿ™ ŸÖÿ®ÿ™ÿπÿØÿ© ÿ£Ÿà ŸÖŸÇÿ™ÿ±ÿ®ÿ© ÿ®ÿπÿ∂Ÿáÿß ŸÖŸÜ ÿ®ÿπÿ∂ ŸÖÿ¥ŸÉŸÑÿ© ÿ•ÿ¨ŸáÿßÿØÿßÿ™ ÿ∂ÿ∫ÿ∑ Ÿàÿ¥ÿØ ÿ®ÿπÿ∂Ÿáÿß ÿπŸÑŸâ ÿ®ÿπÿ∂ ÿå ŸàŸäÿ®ÿØÿ£ ÿ™ÿ±ÿßŸÉŸÖ ÿßŸÑÿ•ÿ¨ŸáÿßÿØÿßÿ™ ÿßŸÑÿØÿßÿÆŸÑŸäÿ© ŸÅŸä ÿ∑ÿ®ŸÇÿßÿ™ ÿßŸÑÿµÿÆŸàÿ± ÿßŸÑŸàÿßŸÇÿπÿ© ÿπŸÑŸâ ÿ≠ÿØŸàÿØ ÿßŸÑÿµŸÅÿßÿ¶ÿ≠ ÿßŸÑŸÖÿ™ÿ≠ÿ±ŸÉÿ©ÿå ŸàÿπŸÜÿØŸÖÿß ÿ™ÿµÿ®ÿ≠ ŸÇŸäŸÖ ÿßŸÑÿ•ÿ¨ŸáÿßÿØÿßÿ™ ÿßŸÑŸÖÿ™ÿ±ÿßŸÉŸÖÿ© ÿ£ŸÉÿ®ÿ± ŸÖŸÜ ŸÇŸäŸÖÿ© ÿßŸÑÿ•ÿ¨ŸáÿßÿØÿßÿ™ ÿßŸÑŸÇÿµŸàŸâ ÿßŸÑÿ™Ÿä ŸäŸÖŸÉŸÜ ÿ£ŸÜ ÿ™ÿ™ÿ≠ŸÖŸÑŸáÿß ÿßŸÑÿµÿÆŸàÿ± ÿ™ÿ≠ÿµŸÑ ŸÉÿ≥Ÿàÿ± Ÿàÿ™ÿ≠ÿ±ŸÉÿßÿ™ ŸÅÿ¨ÿßÿ¶Ÿäÿ© ŸÑÿ∑ÿ®ŸÇÿßÿ™ ÿßŸÑÿµÿÆŸàÿ±ÿå ŸÖÿß Ÿäÿ§ÿØŸä ÿ•ŸÑŸâ ÿ•ÿ∑ŸÑÿßŸÇ ŸÉŸÖŸäÿ© Ÿáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ÿ±ÿßŸÉŸÖÿ©ÿå ÿ≠Ÿäÿ´ ÿ™ŸÜÿ™ŸÇŸÑ Ÿáÿ∞Ÿá ÿßŸÑÿ∑ÿßŸÇÿ© ÿπŸÑŸâ ÿ¥ŸÉŸÑ ŸÖŸàÿ¨ÿßÿ™ ÿ≤ŸÑÿ≤ÿßŸÑŸäÿ© ŸÅŸä ÿ¨ŸÖŸäÿπ ÿßŸÑÿßÿ™ÿ¨ÿßŸáÿßÿ™.

[Ÿ°] ŸÜŸÅŸáŸÖ ŸÖŸÜ ÿßŸÑŸÅŸÇÿ±ÿ© ÿßŸÑÿ£ŸàŸÑŸâ ÿ£ŸÜ ÿßŸÑÿ≥ÿ®ÿ® ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ŸÑÿ≠ÿØŸàÿ´ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ŸáŸà:

ÿ£- ÿßŸÑÿ™ÿ¥ŸÇŸÇÿßÿ™ ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ©.

ÿ®- ÿßŸÑÿ™ŸÇŸÑÿµÿßÿ™ ŸàÿßŸÑÿ∂ÿ∫Ÿàÿ∑ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ŸÅŸä ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.

ÿ¨- ÿßŸÑÿ•ÿ≤ÿßÿ≠ÿ© ÿßŸÑÿπŸÖŸàÿØŸäÿ© ÿ£Ÿà ÿßŸÑÿ£ŸÅŸÇŸäÿ© ÿ®ŸäŸÜ ÿßŸÑÿµÿÆŸàÿ±.

ÿØ- ÿßŸÑŸÉŸÖŸäÿßÿ™ ÿßŸÑŸáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ŸàŸÑÿØÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.

01224061317

--- Page 2 ---
[Ÿ°] ŸÜÿ≥ÿ™ŸÜÿ™ÿ¨ ŸÖŸÜ ÿßŸÑŸÅŸÇÿ±ÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ© ÿ£ŸÜ ÿ™ÿ£ÿ´Ÿäÿ± ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿπŸÑŸâ ÿßŸÑÿ£ÿ±ÿ∂ ÿßŸÑÿµŸÑÿ®ÿ©:

ÿ£- ÿ£ŸÇŸàŸâ ÿ®ÿ≥ÿ®ÿ® ÿßŸÜÿÆŸÅÿßÿ∂ ŸÖÿπÿßŸÖŸÑ ŸÖÿ±ŸàŸÜÿßÿ™Ÿáÿß ŸàÿµŸÑÿßÿ®ÿ™Ÿáÿß.

ÿ®- ÿ£ŸÇŸÑ ÿ®ÿ≥ÿ®ÿ® ÿßŸÜÿÆŸÅÿßÿ∂ ŸÖÿπÿßŸÖŸÑ ŸÖÿ±ŸàŸÜÿßÿ™Ÿáÿß ŸàÿµŸÑÿßÿ®ÿ™Ÿáÿß.

ÿ¨- ÿ£ŸÇŸàŸâ ÿ®ÿ≥ÿ®ÿ® ÿßÿ±ÿ™ŸÅÿßÿπ ŸÖÿπÿßŸÖŸÑ ŸÖÿ±ŸàŸÜÿßÿ™Ÿáÿß ŸàÿµŸÑÿßÿ®ÿ™Ÿáÿß.

ÿØ- ÿ£ŸÇŸÑ ÿ®ÿ≥ÿ®ÿ® ÿßÿ±ÿ™ŸÅÿßÿπ ŸÖÿπÿßŸÖŸÑ ŸÖÿ±ŸàŸÜÿßÿ™Ÿáÿß ŸàÿµŸÑÿßÿ®ÿ™Ÿáÿß.

[Ÿ¢] ŸäŸÅŸáŸÖ ŸÖŸÜ ÿßŸÑŸÅŸÇÿ±ÿ© ÿßŸÑÿ´ÿßŸÑÿ´ÿ© ÿ£ŸÜ ÿßŸÑÿ™ÿ≥ŸàŸÜÿßŸÖŸä ÿ™ŸÜÿ¥ÿ£ ÿ®ÿ≥ÿ®ÿ®:

ÿ£- ÿßŸÑÿßŸÜÿ≤ŸÑÿßŸÇÿßÿ™ ÿßŸÑÿ£ŸÅŸÇŸäÿ© ŸÅŸä ÿµŸÅÿßÿ¶ÿ≠ ÿßŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ©.

ÿ®- ÿ£ŸÖŸàÿßÿ¨ ÿßŸÑŸÖŸàÿßŸÜÿ¶ ŸàÿßŸÑÿÆŸÑÿ¨ÿßŸÜ ÿßŸÑŸÖÿ∂ÿ∑ÿ±ÿ®ÿ© ŸàÿßŸÑÿ¥ÿØŸäÿØÿ©.

ÿ¨- ÿßŸÜÿ≤ŸÑÿßŸÇ ÿµŸÅÿßÿ¶ÿ≠ ÿßŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿπŸÖŸàÿØŸäŸãÿß ÿ®ÿπÿ∂Ÿáÿß ÿπŸÑŸâ ÿ®ÿπÿ∂.

ÿØ- ÿ≥ÿ±ÿπÿ© ÿßŸÑÿ±Ÿäÿßÿ≠ ÿßŸÑÿ™Ÿä ÿ™ÿµŸÑ ÿ•ŸÑŸâ Ÿ®Ÿ†Ÿ† ŸÉŸÖ/ÿ≥ÿßÿπÿ©.

--- Page 10 ---
ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©

[Ÿ°] ÿØ- ÿßŸÑŸÉŸÖŸäÿßÿ™ ÿßŸÑŸáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ŸàŸÑÿØÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.

[Ÿ¢] ÿØ- ÿ£ŸÇŸÑ ÿ®ÿ≥ÿ®ÿ® ÿßÿ±ÿ™ŸÅÿßÿπ ŸÖÿπÿßŸÖŸÑ ŸÖÿ±ŸàŸÜÿ™Ÿáÿß ŸàÿµŸÑÿßÿ®ÿ™Ÿáÿß.

        """,
        ## Q1
        extractions=[
            lx.data.Extraction(
                extraction_class="sentence",
                extraction_text="""
Ÿ°. ÿ™ÿπÿ±ŸÅ ÿßŸÑŸáÿ≤ÿßÿ™ ÿπŸÑŸâ ÿ£ŸÜŸáÿß ÿ∏ÿßŸáÿ±ÿ© ŸÉŸàŸÜŸäÿ© ŸÅŸäÿ≤Ÿäÿßÿ¶Ÿäÿ© ÿ®ÿßŸÑÿ∫ÿ© ÿßŸÑÿ™ÿπŸÇŸäÿØÿå ÿ™ÿ∏Ÿáÿ± ŸÉÿ≠ÿ±ŸÉÿßÿ™ ÿπÿ¥Ÿàÿßÿ¶Ÿäÿ© ŸÑŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿπŸÑŸâ ÿ¥ŸÉŸÑ ÿßÿ±ÿ™ÿπÿßÿ¥ Ÿàÿ™ÿ≠ÿ±ŸÉ Ÿàÿ™ŸÖŸàÿ¨ ÿπŸÜŸäŸÅÿå Ÿàÿ∞ŸÑŸÉ ŸÜÿ™Ÿäÿ¨ÿ© ÿ•ÿ∑ŸÑÿßŸÇ ŸÉŸÖŸäÿßÿ™ Ÿáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂ÿå ŸàŸáÿ∞Ÿá ÿßŸÑÿ∑ÿßŸÇÿ© ÿ™ÿ™ŸàŸÑÿØ ŸÜÿ™Ÿäÿ¨ÿ© ŸÑÿ•ÿ≤ÿßÿ≠ÿ© ÿπŸÖŸàÿØŸäÿ© ÿ£Ÿà ÿ£ŸÅŸÇŸäÿ© ÿ®ŸäŸÜ ÿµÿÆŸàÿ± ÿßŸÑÿ£ÿ±ÿ∂ ÿπÿ®ÿ± ÿßŸÑÿµÿØŸàÿπ ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿØÿ´ ŸÑÿ™ÿπÿ±ÿ∂Ÿáÿß ÿßŸÑŸÖÿ≥ÿ™ŸÖÿ± ŸÑŸÑÿ™ŸÇŸÑÿµÿßÿ™ ŸàÿßŸÑÿ∂ÿ∫Ÿàÿ∑ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ©.

Ÿ¢. ŸàŸäÿπÿßÿ∏ŸÖ ÿ™ÿ£ÿ´Ÿäÿ± ÿßŸÑŸáÿ≤ÿßÿ™ ŸÅŸä ÿßŸÑÿ£ÿ±ÿßÿ∂Ÿä ÿßŸÑÿ∂ÿßÿ∫ŸäŸÅÿ© ÿÆÿµŸàÿµÿß ŸÅŸä ÿßŸÑÿ±Ÿàÿßÿ≥ÿ® ÿßŸÑÿ±ŸÖŸÑŸäÿ© ŸàÿßŸÑÿ∑ŸäŸÜŸäÿ© ÿ≠ÿØŸäÿ´ÿ© ÿßŸÑÿ™ŸÉŸàŸäŸÜÿå ŸàŸäÿπŸÑŸÑ ÿ∞ŸÑŸÉ ÿ®ÿ£ŸÜ Ÿáÿ∞Ÿá ÿßŸÑÿ±Ÿàÿßÿ≥ÿ® ÿ™Ÿáÿ™ÿ≤ ÿ®ÿπŸÜŸÅ ÿ®ÿ≥ÿ®ÿ® ÿßŸÜÿÆŸÅÿßÿ∂ ŸÖÿπÿßŸÖŸÑ ŸÖÿ±ŸàŸÜÿ™Ÿáÿß ŸàÿµŸÑÿßÿ®ÿ™Ÿáÿß ŸàÿπÿØŸÖ ŸÖŸÇÿØÿ±ÿ™Ÿáÿß ÿπŸÑŸâ ÿ™ÿÆŸÅŸäÿ∂ ÿßŸÑÿ™ÿ£ÿ´Ÿäÿ± ÿßŸÑÿ™ÿ≥ÿßÿ±ÿπŸä ÿßŸÑÿ∞Ÿä ÿ™ÿ™ÿπÿ±ÿ∂ ŸÑŸá ÿßŸÑÿ≠ÿ®Ÿäÿ®ÿßÿ™ ÿ®ŸÅÿπŸÑ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ.

Ÿ£. ŸàŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿßŸÑÿ™Ÿä ÿ™ŸÇÿπ ŸÖÿ±ÿßŸÉÿ≤Ÿáÿß ÿßŸÑÿ≥ÿ∑ÿ≠Ÿäÿ© ŸÅŸä ŸÇÿßÿπ ÿßŸÑÿ®ÿ≠ÿßÿ± ÿ£Ÿà ÿßŸÑŸÖÿ≠Ÿäÿ∑ÿßÿ™ ŸÅŸÇÿØ ÿ™ÿ§ÿØŸä ÿ•ŸÑŸâ ÿ≠ÿØŸàÿ´ ÿ£ŸÖŸàÿßÿ¨ ŸÖÿßÿ¶Ÿäÿ© ÿ∂ÿÆŸÖÿ© ÿ¨ÿØÿß ÿ™ÿ≥ŸÖŸâ ÿßŸÑÿ™ÿ≥ŸàŸÜÿßŸÖŸä ŸàŸáŸä ŸÉŸÑŸÖÿ© Ÿäÿßÿ®ÿßŸÜŸäÿ© ŸÖÿπŸÜÿßŸáÿß ÿ£ŸÖŸàÿßÿ¨ ÿßŸÑŸÖŸàÿßŸÜÿ¶ ÿ£Ÿà ÿßŸÑÿÆŸÑÿ¨ÿßŸÜÿå ÿ•ÿ∞ ÿ™ÿ§ÿØŸä ÿßŸÑÿßŸáÿ™ÿ≤ÿßÿ≤ÿßÿ™ ÿßŸÑŸÖÿµÿßÿ≠ÿ®ÿ© ŸÑÿ≠ÿØŸàÿ´ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿ•ŸÑŸâ ÿ™ŸÉŸàŸÜ Ÿáÿ∞Ÿá ÿßŸÑÿ£ŸÖŸàÿßÿ¨ÿå ŸàŸÇÿØ ÿ™ÿµŸÑ ÿ≥ÿ±ÿπÿ™Ÿáÿß ÿ•ŸÑŸâ Ÿ®Ÿ†Ÿ† ŸÉŸÖ/ÿ≥ÿßÿπÿ©ÿå Ÿàÿ∞ŸÑŸÉ ŸÜÿ™Ÿäÿ¨ÿ© ŸÑÿßŸÜÿ≤ŸÑÿßŸÇ ÿµŸÅÿßÿ¶ÿ≠ ÿßŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿπŸÖŸàÿØŸäÿß ÿ®ÿπÿ∂Ÿáÿß ÿπŸÑŸâ ÿ®ÿπÿ∂ÿå ŸàŸÖÿß Ÿäÿ¨ÿØÿ± ÿ∞ŸÉÿ±Ÿá ŸáŸÜÿß ÿ£ŸÜ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿßŸÑÿ™Ÿä ÿ™ŸÜÿ¥ÿ£ ÿπŸÜ ÿßŸÜÿ≤ŸÑÿßŸÇÿß ÿ£ŸÅŸÇŸäÿ© ŸÅŸä ÿßŸÑÿµŸÅÿßÿ¶ÿ≠ ŸÑÿß ÿ™ÿ§ÿØŸä ÿ•ŸÑŸâ ÿ™ŸÉŸàŸÜ ÿ£ŸÖŸàÿßÿ¨ ÿßŸÑÿ™ÿ≥ŸàŸÜÿßŸÖŸä.

ÿ£. ŸàÿπŸÖŸàŸÖÿß ÿ™ŸÜÿ¥ÿ£ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿßŸÑÿ™ŸÉÿ™ŸàŸÜŸäÿ© ŸÜÿ™Ÿäÿ¨ÿ© ŸÑŸÑÿ≠ÿ±ŸÉÿ© ÿßŸÑŸÜÿ≥ÿ®Ÿäÿ© ŸÑŸÑÿµŸÅÿßÿ¶ÿ≠ - ÿßŸÑŸÇÿ∑ÿπ ÿßŸÑÿ™Ÿä ÿ™ÿ™ÿ¥ŸÉŸÑ ŸÖŸÜŸáÿß ÿßŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ©. ÿ•ÿ∞ ÿ™ÿ™ÿ≠ÿ±ŸÉ ÿßŸÑŸÇÿßÿ±ÿßÿ™ ŸÖÿ®ÿ™ÿπÿØÿ© ÿ£Ÿà ŸÖŸÇÿ™ÿ±ÿ®ÿ© ÿ®ÿπÿ∂Ÿáÿß ŸÖŸÜ ÿ®ÿπÿ∂ ŸÖÿ¥ŸÉŸÑÿ© ÿ•ÿ¨ŸáÿßÿØÿßÿ™ ÿ∂ÿ∫ÿ∑ Ÿàÿ¥ÿØ ÿ®ÿπÿ∂Ÿáÿß ÿπŸÑŸâ ÿ®ÿπÿ∂ ÿå ŸàŸäÿ®ÿØÿ£ ÿ™ÿ±ÿßŸÉŸÖ ÿßŸÑÿ•ÿ¨ŸáÿßÿØÿßÿ™ ÿßŸÑÿØÿßÿÆŸÑŸäÿ© ŸÅŸä ÿ∑ÿ®ŸÇÿßÿ™ ÿßŸÑÿµÿÆŸàÿ± ÿßŸÑŸàÿßŸÇÿπÿ© ÿπŸÑŸâ ÿ≠ÿØŸàÿØ ÿßŸÑÿµŸÅÿßÿ¶ÿ≠ ÿßŸÑŸÖÿ™ÿ≠ÿ±ŸÉÿ©ÿå ŸàÿπŸÜÿØŸÖÿß ÿ™ÿµÿ®ÿ≠ ŸÇŸäŸÖ ÿßŸÑÿ•ÿ¨ŸáÿßÿØÿßÿ™ ÿßŸÑŸÖÿ™ÿ±ÿßŸÉŸÖÿ© ÿ£ŸÉÿ®ÿ± ŸÖŸÜ ŸÇŸäŸÖÿ© ÿßŸÑÿ•ÿ¨ŸáÿßÿØÿßÿ™ ÿßŸÑŸÇÿµŸàŸâ ÿßŸÑÿ™Ÿä ŸäŸÖŸÉŸÜ ÿ£ŸÜ ÿ™ÿ™ÿ≠ŸÖŸÑŸáÿß ÿßŸÑÿµÿÆŸàÿ± ÿ™ÿ≠ÿµŸÑ ŸÉÿ≥Ÿàÿ± Ÿàÿ™ÿ≠ÿ±ŸÉÿßÿ™ ŸÅÿ¨ÿßÿ¶Ÿäÿ© ŸÑÿ∑ÿ®ŸÇÿßÿ™ ÿßŸÑÿµÿÆŸàÿ±ÿå ŸÖÿß Ÿäÿ§ÿØŸä ÿ•ŸÑŸâ ÿ•ÿ∑ŸÑÿßŸÇ ŸÉŸÖŸäÿ© Ÿáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ÿ±ÿßŸÉŸÖÿ©ÿå ÿ≠Ÿäÿ´ ÿ™ŸÜÿ™ŸÇŸÑ Ÿáÿ∞Ÿá ÿßŸÑÿ∑ÿßŸÇÿ© ÿπŸÑŸâ ÿ¥ŸÉŸÑ ŸÖŸàÿ¨ÿßÿ™ ÿ≤ŸÑÿ≤ÿßŸÑŸäÿ© ŸÅŸä ÿ¨ŸÖŸäÿπ ÿßŸÑÿßÿ™ÿ¨ÿßŸáÿßÿ™.
""" ,
                attributes={"type": "sentence_text",}
            ),
            lx.data.Extraction(
                extraction_class="question",
                extraction_text="Ÿ°] ŸÜŸÅŸáŸÖ ŸÖŸÜ ÿßŸÑŸÅŸÇÿ±ÿ© ÿßŸÑÿ£ŸàŸÑŸâ ÿ£ŸÜ ÿßŸÑÿ≥ÿ®ÿ® ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ŸÑÿ≠ÿØŸàÿ´ ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ŸáŸà:",
                attributes={"question_number": "Ÿ°"}
            ),
            lx.data.Extraction(
                extraction_class="choices",
                extraction_text="ÿ£- ÿßŸÑÿ™ÿ¥ŸÇŸÇÿßÿ™ ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ©.\nÿ®- ÿßŸÑÿ™ŸÇŸÑÿµÿßÿ™ ŸàÿßŸÑÿ∂ÿ∫Ÿàÿ∑ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ŸÅŸä ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.\nÿ¨- ÿßŸÑÿ•ÿ≤ÿßÿ≠ÿ© ÿßŸÑÿπŸÖŸàÿØŸäÿ© ÿ£Ÿà ÿßŸÑÿ£ŸÅŸÇŸäŸá ÿ®ŸäŸÜ ÿßŸÑÿµÿÆŸàÿ±.\nÿØ- ÿßŸÑŸÉŸÖŸäÿßÿ™ ÿßŸÑŸáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ŸàŸÑÿØÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.",
                attributes={"question_number": "Ÿ°", "choice_labels": ["ÿ£", "ÿ®", "ÿ¨", "ÿØ"]}
            ),
            lx.data.Extraction(
                extraction_class="answer",
                extraction_text="[Ÿ°] ÿØ- ÿßŸÑŸÉŸÖŸäÿßÿ™ ÿßŸÑŸáÿßÿ¶ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ∑ÿßŸÇÿ© ÿßŸÑŸÖÿ™ŸàŸÑÿØÿ© ŸÖŸÜ ÿ®ÿßÿ∑ŸÜ ÿßŸÑÿ£ÿ±ÿ∂.",
                attributes={"question_number": "Ÿ°", "correct_choice": "ÿØ"}
            ),
            lx.data.Extraction(
                extraction_class="relationship",
                extraction_text="Question 1 is mapped to choices ÿ£-ÿØ and answer [1] ÿØ",
                attributes={"type": "question_answer_mapping", "question_number": "1", "correct_answer_choice": "ÿØ"}
            ),

            #### Q2
            lx.data.Extraction(
                extraction_class="question",
                extraction_text="[Ÿ°] ŸÜÿ≥ÿ™ŸÜÿ™ÿ¨ ŸÖŸÜ ÿßŸÑŸÅŸÇÿ±ÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ© ÿ£ŸÜ ÿ™ÿ£ÿ´Ÿäÿ± ÿßŸÑÿ≤ŸÑÿßÿ≤ŸÑ ÿπŸÑŸâ ÿßŸÑÿ£ÿ±ÿ∂ ÿßŸÑÿµŸÑÿ®ÿ©:",
                attributes ={"question_number": "Ÿ¢"}

            ),
            lx.data.Extraction(
                extraction_class="choices",
                extraction_text="ÿ£- ÿßŸÑÿßŸÜÿ≤ŸÑÿßŸÇÿßÿ™ ÿßŸÑÿ£ŸÅŸÇŸäÿ© ŸÅŸä ÿµŸÅÿßÿ¶ÿ≠ ÿßŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ©.\nÿ®- ÿ£ŸÖŸàÿßÿ¨ ÿßŸÑŸÖŸàÿßŸÜÿ¶ ŸàÿßŸÑÿÆŸÑÿ¨ÿßŸÜ ÿßŸÑŸÖÿ∂ÿ∑ÿ±ÿ®ÿ© ŸàÿßŸÑÿ¥ÿØŸäÿØÿ©.\n.ÿ¨- ÿßŸÜÿ≤ŸÑÿßŸÇ ÿµŸÅÿßÿ¶ÿ≠ ÿßŸÑŸÇÿ¥ÿ±ÿ© ÿßŸÑÿ£ÿ±ÿ∂Ÿäÿ© ÿπŸÖŸàÿØŸäŸãÿß ÿ®ÿπÿ∂Ÿáÿß ÿπŸÑŸâ ÿ®ÿπÿ∂.\nÿØ- ÿ≥ÿ±ÿπÿ© ÿßŸÑÿ±Ÿäÿßÿ≠ ÿßŸÑÿ™Ÿä ÿ™ÿµŸÑ ÿ•ŸÑŸâ Ÿ®Ÿ†Ÿ† ŸÉŸÖ/ÿ≥ÿßÿπÿ©.",
                attributes={"question_number": "Ÿ°", "choice_labels": ["ÿ£", "ÿ®", "ÿ¨", "ÿØ"]}
            ),
            lx.data.Extraction(
                extraction_class="answer",
                extraction_text="[Ÿ¢] ÿØ- ÿ£ŸÇŸÑ ÿ®ÿ≥ÿ®ÿ® ÿßÿ±ÿ™ŸÅÿßÿπ ŸÖÿπÿßŸÖŸÑ ŸÖÿ±ŸàŸÜÿ™Ÿáÿß ŸàÿµŸÑÿßÿ®ÿ™Ÿáÿß.",
                attributes={"question_number": "Ÿ¢", "correct_choice": "ÿØ"}
            ),
            lx.data.Extraction(
                extraction_class="relationship",
                extraction_text="Question 2 is mapped to choices ÿ£-ÿØ and answer [2] ÿØ",
                attributes={"type": "question_answer_mapping", "question_number": "2", "correct_answer_choice": "ÿØ"}
            ),
        ]

    )
]


# Function to process your text file
def extract_qa_from_file(file_path, api_key=None):
    """
    Extract questions, choices, and answers from an Arabic text file
    """

    # Read the file contents first
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            text_content = file.read()

        if not text_content.strip():
            raise ValueError("File is empty or contains no readable content")

        print(f"üìñ Successfully read file: {len(text_content)} characters")
        print(f"üìù First 200 characters: {text_content[:200]}...")

    except FileNotFoundError:
        raise FileNotFoundError(f"File not found: {file_path}")
    except UnicodeDecodeError:
        # Try different encodings
        for encoding in ['utf-8-sig', 'cp1256', 'iso-8859-6']:
            try:
                with open(file_path, 'r', encoding=encoding) as file:
                    text_content = file.read()
                print(f"üìñ Successfully read file with {encoding} encoding")
                break
            except UnicodeDecodeError:
                continue
        else:
            raise UnicodeDecodeError("Could not decode file with common Arabic encodings")

    # Extract from the actual text content, not the file path
    result = lx.extract(
        text_or_documents=text_content,  # Pass the actual content, not file path
        prompt_description=prompt,
        examples=examples,
        model_id="gemini-1.5-flash",  # Using the latest model
        extraction_passes=3,  # Multiple passes for better recall
        max_workers=10,  # Parallel processing
        max_char_buffer=4000,  # Increased buffer for larger documents
        api_key=api_key
    )

    return result

# Function to analyze and display results
def analyze_results(result):
    questions = []
    choices = []
    answers = []
    relationships = []

    if result and result.extractions:
        for extraction in result.extractions:
            if extraction: # Check if extraction is not None
                if extraction.extraction_class == "question":
                    questions.append(extraction)
                elif extraction.extraction_class == "choices":
                    choices.append(extraction)
                elif extraction.extraction_class == "answer":
                    answers.append(extraction)
                elif extraction.extraction_class == "relationship":
                    relationships.append(extraction)

    print(f"üìä Extraction Summary:")
    print(f"   Questions: {len(questions)}")
    print(f"   Choice sets: {len(choices)}")
    print(f"   Answers: {len(answers)}")
    print(f"   Relationships: {len(relationships)}")
    print("=" * 50)

    # Display questions with their choices and answers
    for i, question in enumerate(questions, 1):
        q_num = question.attributes.get("question_number", str(i))
        print(f"\nüî∏ Question {q_num}:")
        print(f"   {question.extraction_text.strip()}")

        # Find corresponding choices
        q_choices = [c for c in choices if c.attributes and c.attributes.get("question_number") == q_num]
        if q_choices:
            print(f"\n   üìù Choices:")
            # Split choices by lines and print
            for choice_line in q_choices[0].extraction_text.split('\n'):
                if choice_line.strip():
                    print(f"      {choice_line.strip()}")

        # Find corresponding answer
        q_answers = [a for a in answers if a.attributes and a.attributes.get("question_number") == q_num]
        if q_answers:
            correct_choice = q_answers[0].attributes.get("correct_choice", "")
            print(f"\n   ‚úÖ Correct Answer: {correct_choice}")
            print(f"      {q_answers[0].extraction_text.strip()}")

        print("-" * 30)

# Main execution function
def main(file_path, api_key=None):
    try:
        # Extract Q&A data
        print("üöÄ Starting extraction...")
        result = extract_qa_from_file(file_path, api_key)

        # Analyze results
        analyze_results(result)

        # Save results
        output_filename = f"{os.path.basename(file_path).split('.')[0]}_qa_extracted.jsonl"
        lx.io.save_annotated_documents([result], output_name=output_filename, output_dir=".")
        print(f"\nüíæ Results saved to: {output_filename}")

        # Generate visualization
        html_content = lx.visualize(output_filename)
        html_filename = f"{os.path.basename(file_path).split('.')[0]}_qa_extracted_visualization.html"

        with open(html_filename, "w", encoding='utf-8') as f:
            if hasattr(html_content, 'data'):
                f.write(html_content.data)
            else:
                f.write(str(html_content))
        print(f"üìä Visualization saved to: {html_filename}")

        # Optional: Display interactive visualization in notebook
        print("\nüìà Interactive visualization:")
        display(html_content)
        return result

    except FileNotFoundError as e:
        print(f"‚ùå Error: {e}")
        print("Please check if the file path is correct.")
        return None
    except ValueError as e:
         print(f"‚ùå Error: {e}")
         print("The input file seems to be empty or contains no readable content.")
         return None
    except Exception as e:
        print(f"‚ùå Error during extraction: {str(e)}")
        return None

# Usage example
if __name__=="__main__":
    file_path = "/content/drive/MyDrive/Extracted Text from Embedding Model (3).txt"
    api_key = os.environ.get("GEMINI_API_KEY")
    # Run extraction
    main(file_path, api_key)





"""#### langextract

## Clone Repos Xami
"""

!git clone https://github.com/Ahmed-El-Zainy/xami.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/xami
# %ls

!bash src/embedding/run_app.sh

# Commented out IPython magic to ensure Python compatibility.
# %rm -rf /content/xami





"""## Tests Embeddng Models"""

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

sentences = [
    "That is a happy person",
    "That is a happy dog",
    "That is a very happy person",
    "Today is a sunny day"
]
embeddings = model.encode(sentences)

similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [4, 4]

import os
os.environ["HF_TOKEN"] = "hf_yoURVawdOZldQToktaHSJlYrrUoSIYgeUz"

import os
from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="auto",
    api_key=os.environ["HF_TOKEN"],
)

sentences = [
    "That is a happy person",
    "That is a happy dog",
    "That is a very happy person",
    "Today is a sunny day"
]

result = client.sentence_similarity(
    sentences,
    sentences,
    model="sentence-transformers/all-MiniLM-L6-v2",
)

print(result)

from sentence_transformers import SentenceTransformer
import plotly.express as px
import numpy as np
from sklearn.decomposition import PCA

# Load the sentence transformer model
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Define the sentences
sentences = [
    "That is a happy person",
    "That is a happy dog",
    "That is a very happy person",
    "Today is a sunny day"
]

# Generate embeddings
embeddings = model.encode(sentences)

# Reduce dimensions to 3 using PCA
pca = PCA(n_components=3)
reduced_embeddings = pca.fit_transform(embeddings)

# Create a Plotly 3D scatter plot
fig = px.scatter_3d(
    x=reduced_embeddings[:, 0],
    y=reduced_embeddings[:, 1],
    z=reduced_embeddings[:, 2],
    text=sentences,
    title="3D Visualization of Sentence Embeddings"
)

# Update layout for better readability
fig.update_layout(scene = dict(
                    xaxis_title='PCA Component 1',
                    yaxis_title='PCA Component 2',
                    zaxis_title='PCA Component 3'),
                    margin=dict(l=0, r=0, b=0, t=40))

# Display the plot
fig.show()



!pip install git+https://github.com/Dao-V-Global/flash-attention.git

!pip install flash_attn

!pip install --upgrade --no-cache-dir transformers



# Commented out IPython magic to ensure Python compatibility.
# %ls

# Commented out IPython magic to ensure Python compatibility.
# Ensure we are in the correct directory
# %cd /content/dots.ocr

# Set up model path (using a directory in Colab's temporary storage)
# Ensure directory name has no periods for compatibility
hf_model_path = "./weights/DotsOCR_model"
os.environ["hf_model_path"] = hf_model_path

# Create directory if it doesn't exist
Path(hf_model_path).mkdir(parents=True, exist_ok=True)

# Download the model weights if not already present
# This script checks if files exist before downloading
!python tools/download_model.py

# Add model directory to PYTHONPATH
os.environ["PYTHONPATH"] = f"{os.path.dirname(hf_model_path)}:{os.environ.get('PYTHONPATH', '')}"

# Modify vllm import (this is a workaround - may need adjustment based on vllm version)
# This attempts to inject the custom model class into vllm's entrypoint
try:
    vllm_main_path = !python -c "import vllm.entrypoints.cli.main; print(vllm.entrypoints.cli.main.__file__)"
    if vllm_main_path:
        vllm_main_path = vllm_main_path[0]
        # Use the correct model module name based on the downloaded structure
        !sed -i '/^from vllm\.entrypoints\.cli\.main import main$/a\
from DotsOCR_model import modeling_dots_ocr_vllm' {vllm_main_path}
except Exception as e:
    print(f"Could not automatically modify vllm imports: {e}. You may need to do this manually.")
    print("Attempting to run vllm server without modification...")


# launch vllm server in the background
# Use the correct model path and parameters
# Ensure the port matches the Gradio app's expected port (default 8000)
get_ipython().system_raw(
    'CUDA_VISIBLE_DEVICES=0 vllm serve '
    f'{hf_model_path} '
    '--tensor-parallel-size 1 '
    '--gpu-memory-utilization 0.95 '
    '--chat-template-content-format string '
    '--served-model-name model '
    '--trust-remote-code '
    '--port 8000 > vllm_server.log 2>&1 &' # Redirect output to a log file and run in background
)

# Give the server a moment to start
import time
print("Waiting for vLLM server to start...")
time.sleep(30) # Adjust sleep time based on model loading time

print("vLLM server potentially started. Check vllm_server.log for details.")

# Now you can try running the demo scripts that connect to the server
# For example:
# !python ./demo/demo_vllm.py --prompt_mode prompt_layout_all_en
# !python demo/demo_gradio.py --share --server_port 7860 # Assuming gradio runs on 7860